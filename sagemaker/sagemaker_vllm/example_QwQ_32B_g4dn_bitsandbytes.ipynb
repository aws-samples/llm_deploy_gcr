{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SageMaker VLLM endpoint example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Define some variables\n",
    "\n",
    "The byoc will build and store a vllm endpoint docker image in you ECR private repo (for example `sagemaker_endpoint/vllm`), you need to define the following variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"Qwen/QwQ-32B\"\n",
    "INSTANCE_TYPE = \"ml.g4dn.12xlarge\"\n",
    "VLLM_VERSION = \"v0.7.3\"\n",
    "REPO_NAMESPACE = \"sagemaker_endpoint/vllm\"\n",
    "ACCOUNT = !aws sts get-caller-identity --query Account --output text\n",
    "REGION = !aws configure get region\n",
    "ACCOUNT = ACCOUNT[0]\n",
    "REGION = REGION[0]\n",
    "if REGION.startswith(\"cn\"):\n",
    "    # this is a example repo port from vllm/vllm-openai, you can create your own docker image in your global region account\n",
    "    VLLM_REPO = \"public.ecr.aws/y0a9p9k0/vllm/vllm-openai\"\n",
    "    CONTAINER = f\"{ACCOUNT}.dkr.ecr.{REGION}.amazonaws.com.cn/{REPO_NAMESPACE}:{VLLM_VERSION}\"\n",
    "else:\n",
    "    VLLM_REPO = \"vllm/vllm-openai\"\n",
    "    CONTAINER = f\"{ACCOUNT}.dkr.ecr.{REGION}.amazonaws.com/{REPO_NAMESPACE}:{VLLM_VERSION}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the container\n",
    "\n",
    "Endpoint starting codes are in `app/`. The script will build and push to ecr. \n",
    "\n",
    "**The docker only need to be built once**, and after that, when deploying other endpoints, the same docker image can be shared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runging: VLLM_REPO=vllm/vllm-openai VLLM_VERSION=v0.7.3 REPO_NAMESPACE=sagemaker_endpoint/vllm ACCOUNT=596899493901 REGION=us-east-1 bash ./build_and_push.sh\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "596899493901.dkr.ecr.us-east-1.amazonaws.com/sagemaker_endpoint/vllm:v0.7.3\n",
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                          docker:default\n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.1s (2/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   0.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.8s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   0.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.9s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   0.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.1s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   0.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.2s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   1.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.4s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   1.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.4s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   1.2s\n",
      "\u001b[2m => => # Collecting s5cmd                                                      \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.5s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   1.3s\n",
      "\u001b[2m => => # l/lib/python3.12/dist-packages (from boto3) (0.11.2)                  \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/loc\n",
      "\u001b[0m\u001b[2m => => # al/lib/python3.12/dist-packages (from botocore>=1.34.153->sagemaker-ss\n",
      "\u001b[0m\u001b[2m => => # h-helper) (2.9.0.post0)                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting sagemaker-core<3.0.0,>=2.0.0 (from sagemaker>=2.145.0->sage\n",
      "\u001b[0m\u001b[2m => => # maker-ssh-helper)                                                     \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.6s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   1.5s\n",
      "\u001b[2m => => # >=2.145.0->sagemaker-ssh-helper) (13.9.4)                             \n",
      "\u001b[0m\u001b[2m => => # Collecting mock<5.0,>4.0 (from sagemaker-core<3.0.0,>=2.0.0->sagemaker\n",
      "\u001b[0m\u001b[2m => => # >=2.145.0->sagemaker-ssh-helper)                                      \n",
      "\u001b[0m\u001b[2m => => #   Downloading mock-4.0.3-py3-none-any.whl.metadata (2.8 kB)           \n",
      "\u001b[0m\u001b[2m => => # Collecting importlib-metadata<7.0,>=1.4.0 (from sagemaker-core<3.0.0,>\n",
      "\u001b[0m\u001b[2m => => # =2.0.0->sagemaker>=2.145.0->sagemaker-ssh-helper)                     \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.8s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   1.6s\n",
      "\u001b[2m => => # .0->sagemaker-ssh-helper) (25.1.0)                                    \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: packaging>=20.0 in /usr/local/lib/pytho\n",
      "\u001b[0m\u001b[2m => => # n3.12/dist-packages (from sagemaker-core<3.0.0,>=2.0.0->sagemaker>=2.1\n",
      "\u001b[0m\u001b[2m => => # 45.0->sagemaker-ssh-helper) (24.2)                                    \n",
      "\u001b[0m\u001b[2m => => # Collecting protobuf<5.0,>=3.12 (from sagemaker-core<3.0.0,>=2.0.0->sag\n",
      "\u001b[0m\u001b[2m => => # emaker>=2.145.0->sagemaker-ssh-helper)                                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.9s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   1.7s\n",
      "\u001b[2m => => # >=2.145.0->sagemaker-ssh-helper)                                      \n",
      "\u001b[0m\u001b[2m => => #   Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux\n",
      "\u001b[0m\u001b[2m => => # _2_28_x86_64.whl.metadata (91 kB)                                     \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.\n",
      "\u001b[0m\u001b[2m => => # 12/dist-packages (from sagemaker-core<3.0.0,>=2.0.0->sagemaker>=2.145.\n",
      "\u001b[0m\u001b[2m => => # 0->sagemaker-ssh-helper) (1.26.4)                                     \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.0s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   1.8s\n",
      "\u001b[2m => => #   Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)      \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.\n",
      "\u001b[0m\u001b[2m => => # 12/dist-packages (from sagemaker-core<3.0.0,>=2.0.0->sagemaker>=2.145.\n",
      "\u001b[0m\u001b[2m => => # 0->sagemaker-ssh-helper) (2.5.1)                                      \n",
      "\u001b[0m\u001b[2m => => # Collecting scipy>=1.5.0 (from sagemaker-core<3.0.0,>=2.0.0->sagemaker>\n",
      "\u001b[0m\u001b[2m => => # =2.145.0->sagemaker-ssh-helper)                                       \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.1s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   1.9s\n",
      "\u001b[2m => => # ssh-helper) (8.3.4)                                                   \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-\n",
      "\u001b[0m\u001b[2m => => # packages (from sagemaker-serve<2.0.0->sagemaker>=2.145.0->sagemaker-ss\n",
      "\u001b[0m\u001b[2m => => # h-helper) (4.67.1)                                                    \n",
      "\u001b[0m\u001b[2m => => # Collecting tritonclient[http] (from sagemaker-serve<2.0.0->sagemaker>=\n",
      "\u001b[0m\u001b[2m => => # 2.145.0->sagemaker-ssh-helper)                                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.2s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   2.1s\n",
      "\u001b[2m => => # ->sagemaker-ssh-helper)                                               \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.man\n",
      "\u001b[0m\u001b[2m => => # ylinux_2_28_x86_64.whl.metadata (5.1 kB)                              \n",
      "\u001b[0m\u001b[2m => => # Collecting graphene<4,>=3 (from sagemaker-train<2.0.0->sagemaker>=2.14\n",
      "\u001b[0m\u001b[2m => => # 5.0->sagemaker-ssh-helper)                                            \n",
      "\u001b[0m\u001b[2m => => #   Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.3s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   2.2s\n",
      "\u001b[2m => => # =2.0.0->sagemaker>=2.145.0->sagemaker-ssh-helper) (0.36.2)            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python\n",
      "\u001b[0m\u001b[2m => => # 3.12/dist-packages (from jsonschema<5.0.0->sagemaker-core<3.0.0,>=2.0.\n",
      "\u001b[0m\u001b[2m => => # 0->sagemaker>=2.145.0->sagemaker-ssh-helper) (0.23.0)                 \n",
      "\u001b[0m\u001b[2m => => # Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.1.0->sagem\n",
      "\u001b[0m\u001b[2m => => # aker-core<3.0.0,>=2.0.0->sagemaker>=2.145.0->sagemaker-ssh-helper)    \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.5s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   2.3s\n",
      "\u001b[2m => => # 3.12/dist-packages (from jsonschema<5.0.0->sagemaker-core<3.0.0,>=2.0.\n",
      "\u001b[0m\u001b[2m => => # 0->sagemaker>=2.145.0->sagemaker-ssh-helper) (0.23.0)                 \n",
      "\u001b[0m\u001b[2m => => # Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.1.0->sagem\n",
      "\u001b[0m\u001b[2m => => # aker-core<3.0.0,>=2.0.0->sagemaker>=2.145.0->sagemaker-ssh-helper)    \n",
      "\u001b[0m\u001b[2m => => #   Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)            \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (setup.py): started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.7s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   2.5s\n",
      "\u001b[2m => => # 3.12/dist-packages (from jsonschema<5.0.0->sagemaker-core<3.0.0,>=2.0.\n",
      "\u001b[0m\u001b[2m => => # 0->sagemaker>=2.145.0->sagemaker-ssh-helper) (0.23.0)                 \n",
      "\u001b[0m\u001b[2m => => # Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.1.0->sagem\n",
      "\u001b[0m\u001b[2m => => # aker-core<3.0.0,>=2.0.0->sagemaker>=2.145.0->sagemaker-ssh-helper)    \n",
      "\u001b[0m\u001b[2m => => #   Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)            \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (setup.py): started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.8s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   2.6s\n",
      "\u001b[2m => => # 3.12/dist-packages (from jsonschema<5.0.0->sagemaker-core<3.0.0,>=2.0.\n",
      "\u001b[0m\u001b[2m => => # 0->sagemaker>=2.145.0->sagemaker-ssh-helper) (0.23.0)                 \n",
      "\u001b[0m\u001b[2m => => # Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.1.0->sagem\n",
      "\u001b[0m\u001b[2m => => # aker-core<3.0.0,>=2.0.0->sagemaker>=2.145.0->sagemaker-ssh-helper)    \n",
      "\u001b[0m\u001b[2m => => #   Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)            \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (setup.py): started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.0s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   2.8s\n",
      "\u001b[2m => => # 3.12/dist-packages (from jsonschema<5.0.0->sagemaker-core<3.0.0,>=2.0.\n",
      "\u001b[0m\u001b[2m => => # 0->sagemaker>=2.145.0->sagemaker-ssh-helper) (0.23.0)                 \n",
      "\u001b[0m\u001b[2m => => # Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.1.0->sagem\n",
      "\u001b[0m\u001b[2m => => # aker-core<3.0.0,>=2.0.0->sagemaker>=2.145.0->sagemaker-ssh-helper)    \n",
      "\u001b[0m\u001b[2m => => #   Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)            \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (setup.py): started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.1s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   2.9s\n",
      "\u001b[2m => => # 0->sagemaker>=2.145.0->sagemaker-ssh-helper) (0.23.0)                 \n",
      "\u001b[0m\u001b[2m => => # Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.1.0->sagem\n",
      "\u001b[0m\u001b[2m => => # aker-core<3.0.0,>=2.0.0->sagemaker>=2.145.0->sagemaker-ssh-helper)    \n",
      "\u001b[0m\u001b[2m => => #   Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)            \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (setup.py): started                              \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (setup.py): finished with status 'done'          \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.2s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   3.0s\n",
      "\u001b[2m => => # Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib\n",
      "\u001b[0m\u001b[2m => => # /python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core\n",
      "\u001b[0m\u001b[2m => => # <3.0.0,>=2.0.0->sagemaker>=2.145.0->sagemaker-ssh-helper) (2.27.2)    \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib\n",
      "\u001b[0m\u001b[2m => => # /python3.12/dist-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<3\n",
      "\u001b[0m\u001b[2m => => # .0.0,>=2.0.0->sagemaker>=2.145.0->sagemaker-ssh-helper) (3.0.0)       \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.3s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   3.1s\n",
      "\u001b[2m => => # Collecting mlflow-skinny==3.6.0 (from mlflow->sagemaker-serve<2.0.0->s\n",
      "\u001b[0m\u001b[2m => => # agemaker>=2.145.0->sagemaker-ssh-helper)                              \n",
      "\u001b[0m\u001b[2m => => #   Downloading mlflow_skinny-3.6.0-py3-none-any.whl.metadata (31 kB)   \n",
      "\u001b[0m\u001b[2m => => # Collecting mlflow-tracing==3.6.0 (from mlflow->sagemaker-serve<2.0.0->\n",
      "\u001b[0m\u001b[2m => => # sagemaker>=2.145.0->sagemaker-ssh-helper)                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading mlflow_tracing-3.6.0-py3-none-any.whl.metadata (19 kB)  \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.5s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   3.3s\n",
      "\u001b[2m => => #   Downloading flask-3.1.2-py3-none-any.whl.metadata (3.2 kB)          \n",
      "\u001b[0m\u001b[2m => => # Collecting alembic!=1.10.0,<2 (from mlflow->sagemaker-serve<2.0.0->sag\n",
      "\u001b[0m\u001b[2m => => # emaker>=2.145.0->sagemaker-ssh-helper)                                \n",
      "\u001b[0m\u001b[2m => => #   Downloading alembic-1.17.2-py3-none-any.whl.metadata (7.2 kB)       \n",
      "\u001b[0m\u001b[2m => => # Collecting cryptography>=3.3 (from paramiko>=2.11.0->sagemaker-train<2\n",
      "\u001b[0m\u001b[2m => => # .0.0->sagemaker>=2.145.0->sagemaker-ssh-helper)                       \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.6s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   3.4s\n",
      "\u001b[2m => => #   Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)      \n",
      "\u001b[0m\u001b[2m => => # Collecting huey<3,>=2.5.0 (from mlflow->sagemaker-serve<2.0.0->sagemak\n",
      "\u001b[0m\u001b[2m => => # er>=2.145.0->sagemaker-ssh-helper)                                    \n",
      "\u001b[0m\u001b[2m => => #   Downloading huey-2.5.4-py3-none-any.whl.metadata (4.6 kB)           \n",
      "\u001b[0m\u001b[2m => => # Collecting matplotlib<4 (from mlflow->sagemaker-serve<2.0.0->sagemaker\n",
      "\u001b[0m\u001b[2m => => # >=2.145.0->sagemaker-ssh-helper)                                      \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.7s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   3.6s\n",
      "\u001b[2m => => # Collecting pyarrow<23,>=4.0.0 (from mlflow->sagemaker-serve<2.0.0->sag\n",
      "\u001b[0m\u001b[2m => => # emaker>=2.145.0->sagemaker-ssh-helper)                                \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.met\n",
      "\u001b[0m\u001b[2m => => # adata (3.2 kB)                                                        \n",
      "\u001b[0m\u001b[2m => => # Collecting scikit-learn<2 (from mlflow->sagemaker-serve<2.0.0->sagemak\n",
      "\u001b[0m\u001b[2m => => # er>=2.145.0->sagemaker-ssh-helper)                                    \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.9s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   3.7s\n",
      "\u001b[2m => => #   Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.met\n",
      "\u001b[0m\u001b[2m => => # adata (3.2 kB)                                                        \n",
      "\u001b[0m\u001b[2m => => # Collecting scikit-learn<2 (from mlflow->sagemaker-serve<2.0.0->sagemak\n",
      "\u001b[0m\u001b[2m => => # er>=2.145.0->sagemaker-ssh-helper)                                    \n",
      "\u001b[0m\u001b[2m => => #   Downloading scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.many\n",
      "\u001b[0m\u001b[2m => => # linux_2_17_x86_64.whl.metadata (11 kB)                                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.0s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   3.8s\n",
      "\u001b[2m => => # Collecting scikit-learn<2 (from mlflow->sagemaker-serve<2.0.0->sagemak\n",
      "\u001b[0m\u001b[2m => => # er>=2.145.0->sagemaker-ssh-helper)                                    \n",
      "\u001b[0m\u001b[2m => => #   Downloading scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.many\n",
      "\u001b[0m\u001b[2m => => # linux_2_17_x86_64.whl.metadata (11 kB)                                \n",
      "\u001b[0m\u001b[2m => => # Collecting sqlalchemy<3,>=1.4.0 (from mlflow->sagemaker-serve<2.0.0->s\n",
      "\u001b[0m\u001b[2m => => # agemaker>=2.145.0->sagemaker-ssh-helper)                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.1s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   3.9s\n",
      "\u001b[2m => => # 0->sagemaker>=2.145.0->sagemaker-ssh-helper) (0.115.8)                \n",
      "\u001b[0m\u001b[2m => => # Collecting gitpython<4,>=3.1.9 (from mlflow-skinny==3.6.0->mlflow->sag\n",
      "\u001b[0m\u001b[2m => => # emaker-serve<2.0.0->sagemaker>=2.145.0->sagemaker-ssh-helper)         \n",
      "\u001b[0m\u001b[2m => => #   Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)      \n",
      "\u001b[0m\u001b[2m => => # Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.6.0->mlf\n",
      "\u001b[0m\u001b[2m => => # low->sagemaker-serve<2.0.0->sagemaker>=2.145.0->sagemaker-ssh-helper) \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.2s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   4.0s\n",
      "\u001b[2m => => # dist-packages (from mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.\n",
      "\u001b[0m\u001b[2m => => # 0->sagemaker>=2.145.0->sagemaker-ssh-helper) (0.34.0)                 \n",
      "\u001b[0m\u001b[2m => => # Collecting ml_dtypes>=0.5.0 (from onnx->sagemaker-serve<2.0.0->sagemak\n",
      "\u001b[0m\u001b[2m => => # er>=2.145.0->sagemaker-ssh-helper)                                    \n",
      "\u001b[0m\u001b[2m => => #   Downloading ml_dtypes-0.5.4-cp312-cp312-manylinux_2_27_x86_64.manyli\n",
      "\u001b[0m\u001b[2m => => # nux_2_28_x86_64.whl.metadata (8.9 kB)                                 \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.3s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   4.1s\n",
      "\u001b[2m => => # Collecting perf-analyzer (from tritonclient[http]->sagemaker-serve<2.0\n",
      "\u001b[0m\u001b[2m => => # .0->sagemaker>=2.145.0->sagemaker-ssh-helper)                         \n",
      "\u001b[0m\u001b[2m => => #   Downloading perf_analyzer-0.1.0-py3-none-any.whl.metadata (135 bytes\n",
      "\u001b[0m\u001b[2m => => # )                                                                     \n",
      "\u001b[0m\u001b[2m => => # Collecting python-rapidjson>=0.9.1 (from tritonclient[http]->sagemaker\n",
      "\u001b[0m\u001b[2m => => # -serve<2.0.0->sagemaker>=2.145.0->sagemaker-ssh-helper)               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.4s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   4.2s\n",
      "\u001b[2m => => # Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/pyt\n",
      "\u001b[0m\u001b[2m => => # hon3.12/dist-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[http]-\n",
      "\u001b[0m\u001b[2m => => # >sagemaker-serve<2.0.0->sagemaker>=2.145.0->sagemaker-ssh-helper) (1.1\n",
      "\u001b[0m\u001b[2m => => # 8.3)                                                                  \n",
      "\u001b[0m\u001b[2m => => # Collecting Mako (from alembic!=1.10.0,<2->mlflow->sagemaker-serve<2.0.\n",
      "\u001b[0m\u001b[2m => => # 0->sagemaker>=2.145.0->sagemaker-ssh-helper)                          \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.5s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   4.3s\n",
      "\u001b[2m => => # 17_x86_64.whl.metadata (2.6 kB)                                       \n",
      "\u001b[0m\u001b[2m => => # Collecting blinker>=1.9.0 (from Flask<4->mlflow->sagemaker-serve<2.0.0\n",
      "\u001b[0m\u001b[2m => => # ->sagemaker>=2.145.0->sagemaker-ssh-helper)                           \n",
      "\u001b[0m\u001b[2m => => #   Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)        \n",
      "\u001b[0m\u001b[2m => => # Collecting itsdangerous>=2.2.0 (from Flask<4->mlflow->sagemaker-serve<\n",
      "\u001b[0m\u001b[2m => => # 2.0.0->sagemaker>=2.145.0->sagemaker-ssh-helper)                      \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.7s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   4.5s\n",
      "\u001b[2m => => # gemaker>=2.145.0->sagemaker-ssh-helper) (3.0.2)                       \n",
      "\u001b[0m\u001b[2m => => # Collecting werkzeug>=3.1.0 (from Flask<4->mlflow->sagemaker-serve<2.0.\n",
      "\u001b[0m\u001b[2m => => # 0->sagemaker>=2.145.0->sagemaker-ssh-helper)                          \n",
      "\u001b[0m\u001b[2m => => #   Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)       \n",
      "\u001b[0m\u001b[2m => => # Collecting gevent (from geventhttpclient>=2.3.3->tritonclient[http]->s\n",
      "\u001b[0m\u001b[2m => => # agemaker-serve<2.0.0->sagemaker>=2.145.0->sagemaker-ssh-helper)       \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.8s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   4.6s\n",
      "\u001b[2m => => # Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12\n",
      "\u001b[0m\u001b[2m => => # /dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.0.0->sage\n",
      "\u001b[0m\u001b[2m => => # maker-core<3.0.0,>=2.0.0->sagemaker>=2.145.0->sagemaker-ssh-helper) (0\n",
      "\u001b[0m\u001b[2m => => # .1.2)                                                                 \n",
      "\u001b[0m\u001b[2m => => # Collecting contourpy>=1.0.1 (from matplotlib<4->mlflow->sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e<2.0.0->sagemaker>=2.145.0->sagemaker-ssh-helper)                    \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.9s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   4.7s\n",
      "\u001b[2m => => # nux_2_28_x86_64.whl.metadata (5.5 kB)                                 \n",
      "\u001b[0m\u001b[2m => => # Collecting cycler>=0.10 (from matplotlib<4->mlflow->sagemaker-serve<2.\n",
      "\u001b[0m\u001b[2m => => # 0.0->sagemaker>=2.145.0->sagemaker-ssh-helper)                        \n",
      "\u001b[0m\u001b[2m => => #   Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)        \n",
      "\u001b[0m\u001b[2m => => # Collecting fonttools>=4.22.0 (from matplotlib<4->mlflow->sagemaker-ser\n",
      "\u001b[0m\u001b[2m => => # ve<2.0.0->sagemaker>=2.145.0->sagemaker-ssh-helper)                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.0s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   4.8s\n",
      "\u001b[2m => => # aker>=2.145.0->sagemaker-ssh-helper) (11.1.0)                         \n",
      "\u001b[0m\u001b[2m => => # Collecting pyparsing>=3 (from matplotlib<4->mlflow->sagemaker-serve<2.\n",
      "\u001b[0m\u001b[2m => => # 0.0->sagemaker>=2.145.0->sagemaker-ssh-helper)                        \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)      \n",
      "\u001b[0m\u001b[2m => => # Collecting joblib>=1.2.0 (from scikit-learn<2->mlflow->sagemaker-serve\n",
      "\u001b[0m\u001b[2m => => # <2.0.0->sagemaker>=2.145.0->sagemaker-ssh-helper)                     \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.1s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   5.0s\n",
      "\u001b[2m => => #   Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)         \n",
      "\u001b[0m\u001b[2m => => # Collecting threadpoolctl>=3.1.0 (from scikit-learn<2->mlflow->sagemake\n",
      "\u001b[0m\u001b[2m => => # r-serve<2.0.0->sagemaker>=2.145.0->sagemaker-ssh-helper)              \n",
      "\u001b[0m\u001b[2m => => #   Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)   \n",
      "\u001b[0m\u001b[2m => => # Collecting greenlet>=1 (from sqlalchemy<3,>=1.4.0->mlflow->sagemaker-s\n",
      "\u001b[0m\u001b[2m => => # erve<2.0.0->sagemaker>=2.145.0->sagemaker-ssh-helper)                 \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.3s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   5.1s\n",
      "\u001b[2m => => # .11.0->sagemaker-train<2.0.0->sagemaker>=2.145.0->sagemaker-ssh-helper\n",
      "\u001b[0m\u001b[2m => => # )                                                                     \n",
      "\u001b[0m\u001b[2m => => #   Downloading pycparser-2.23-py3-none-any.whl.metadata (993 bytes)    \n",
      "\u001b[0m\u001b[2m => => # Collecting google-auth~=2.0 (from databricks-sdk<1,>=0.20.0->mlflow-sk\n",
      "\u001b[0m\u001b[2m => => # inny==3.6.0->mlflow->sagemaker-serve<2.0.0->sagemaker>=2.145.0->sagema\n",
      "\u001b[0m\u001b[2m => => # ker-ssh-helper)                                                       \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.4s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   5.2s\n",
      "\u001b[2m => => # .6.0->mlflow->sagemaker-serve<2.0.0->sagemaker>=2.145.0->sagemaker-ssh\n",
      "\u001b[0m\u001b[2m => => # -helper)                                                              \n",
      "\u001b[0m\u001b[2m => => #   Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)         \n",
      "\u001b[0m\u001b[2m => => # INFO: pip is looking at multiple versions of opentelemetry-proto to de\n",
      "\u001b[0m\u001b[2m => => # termine which version is compatible with other requirements. This coul\n",
      "\u001b[0m\u001b[2m => => # d take a while.                                                       \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.5s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   5.3s\n",
      "\u001b[2m => => # INFO: This is taking longer than usual. You might need to provide the \n",
      "\u001b[0m\u001b[2m => => # dependency resolver with stricter constraints to reduce runtime. See h\n",
      "\u001b[0m\u001b[2m => => # ttps://pip.pypa.io/warnings/backtracking for guidance. If you want to \n",
      "\u001b[0m\u001b[2m => => # abort this run, press Ctrl + C.                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.\n",
      "\u001b[0m\u001b[2m => => # 3 kB)                                                                 \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.7s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   5.5s\n",
      "\u001b[2m => => # Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/d\n",
      "\u001b[0m\u001b[2m => => # ist-packages (from uvicorn<1->mlflow-skinny==3.6.0->mlflow->sagemaker-\n",
      "\u001b[0m\u001b[2m => => # serve<2.0.0->sagemaker>=2.145.0->sagemaker-ssh-helper) (0.14.0)       \n",
      "\u001b[0m\u001b[2m => => # Collecting zope.event (from gevent->geventhttpclient>=2.3.3->tritoncli\n",
      "\u001b[0m\u001b[2m => => # ent[http]->sagemaker-serve<2.0.0->sagemaker>=2.145.0->sagemaker-ssh-he\n",
      "\u001b[0m\u001b[2m => => # lper)                                                                 \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.8s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   5.6s\n",
      "\u001b[2m => => # >mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.0->sagemaker>=2.145\n",
      "\u001b[0m\u001b[2m => => # .0->sagemaker-ssh-helper)                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)          \n",
      "\u001b[0m\u001b[2m => => # Collecting pyasn1-modules>=0.2.1 (from google-auth~=2.0->databricks-sd\n",
      "\u001b[0m\u001b[2m => => # k<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.0->sag\n",
      "\u001b[0m\u001b[2m => => # emaker>=2.145.0->sagemaker-ssh-helper)                                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.9s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   5.7s\n",
      "\u001b[2m => => # maker-serve<2.0.0->sagemaker>=2.145.0->sagemaker-ssh-helper)          \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)         \n",
      "\u001b[0m\u001b[2m => => # Downloading s5cmd-0.3.3-py3-none-manylinux1_x86_64.manylinux_2_28_x86_\n",
      "\u001b[0m\u001b[2m => => # 64.manylinux_2_5_x86_64.whl (5.0 MB)                                  \n",
      "\u001b[0m\u001b[2m => => #     5.0/5.0 MB 397.2 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 6.0s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   5.8s\n",
      "\u001b[2m => => # Downloading paramiko-4.0.0-py3-none-any.whl (223 kB)                  \n",
      "\u001b[0m\u001b[2m => => # Downloading platformdirs-4.5.0-py3-none-any.whl (18 kB)               \n",
      "\u001b[0m\u001b[2m => => # Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB\n",
      "\u001b[0m\u001b[2m => => # )                                                                     \n",
      "\u001b[0m\u001b[2m => => # Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)                 \n",
      "\u001b[0m\u001b[2m => => # Downloading schema-0.7.8-py2.py3-none-any.whl (19 kB)                 \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 6.1s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   6.0s\n",
      "\u001b[2m => => # Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB\n",
      "\u001b[0m\u001b[2m => => # )                                                                     \n",
      "\u001b[0m\u001b[2m => => # Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)                 \n",
      "\u001b[0m\u001b[2m => => # Downloading schema-0.7.8-py2.py3-none-any.whl (19 kB)                 \n",
      "\u001b[0m\u001b[2m => => # Downloading scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_\n",
      "\u001b[0m\u001b[2m => => # 17_x86_64.whl (35.7 MB)                                               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 6.2s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   6.0s\n",
      "\u001b[2m => => # Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)                 \n",
      "\u001b[0m\u001b[2m => => # Downloading schema-0.7.8-py2.py3-none-any.whl (19 kB)                 \n",
      "\u001b[0m\u001b[2m => => # Downloading scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_\n",
      "\u001b[0m\u001b[2m => => # 17_x86_64.whl (35.7 MB)                                               \n",
      "\u001b[0m\u001b[2m => => #     35.7/35.7 MB 217.6 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 6.3s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   6.1s\n",
      "\u001b[2m => => #     1.3/1.3 MB 187.6 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Downloading onnx-1.19.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_1\n",
      "\u001b[0m\u001b[2m => => # 7_x86_64.whl (18.2 MB)                                                \n",
      "\u001b[0m\u001b[2m => => #     18.2/18.2 MB 279.5 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 6.4s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   6.2s\n",
      "\u001b[2m => => # Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB) \n",
      "\u001b[0m\u001b[2m => => # Downloading cryptography-46.0.3-cp311-abi3-manylinux_2_34_x86_64.whl (\n",
      "\u001b[0m\u001b[2m => => # 4.5 MB)                                                               \n",
      "\u001b[0m\u001b[2m => => #     4.5/4.5 MB 535.9 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Downloading docker-7.1.0-py3-none-any.whl (147 kB)                    \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 6.7s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   6.5s\n",
      "\u001b[2m => => # x_2_28_x86_64.whl (5.0 MB)                                            \n",
      "\u001b[0m\u001b[2m => => #     5.0/5.0 MB 294.6 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Downloading orderly_set-5.5.0-py3-none-any.whl (13 kB)                \n",
      "\u001b[0m\u001b[2m => => # Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7\n",
      "\u001b[0m\u001b[2m => => #  MB)                                                                  \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 6.7s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   6.5s\n",
      "\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Downloading orderly_set-5.5.0-py3-none-any.whl (13 kB)                \n",
      "\u001b[0m\u001b[2m => => # Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7\n",
      "\u001b[0m\u001b[2m => => #  MB)                                                                  \n",
      "\u001b[0m\u001b[2m => => #     47.7/47.7 MB 271.4 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 6.8s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   6.6s\n",
      "\u001b[2m => => # Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)          \n",
      "\u001b[0m\u001b[2m => => # Downloading perf_analyzer-0.1.0-py3-none-any.whl (2.3 kB)             \n",
      "\u001b[0m\u001b[2m => => # Downloading tritonclient-2.62.0-py3-none-manylinux1_x86_64.whl (111 kB\n",
      "\u001b[0m\u001b[2m => => # )                                                                     \n",
      "\u001b[0m\u001b[2m => => # Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)                   \n",
      "\u001b[0m\u001b[2m => => # Downloading cachetools-6.2.2-py3-none-any.whl (11 kB)                 \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 6.9s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   6.7s\n",
      "\u001b[2m => => # Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)                 \n",
      "\u001b[0m\u001b[2m => => # Downloading sqlparse-0.5.3-py3-none-any.whl (44 kB)                   \n",
      "\u001b[0m\u001b[2m => => # Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)              \n",
      "\u001b[0m\u001b[2m => => # Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)                  \n",
      "\u001b[0m\u001b[2m => => # Downloading brotli-1.2.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_\n",
      "\u001b[0m\u001b[2m => => # 17_x86_64.whl (1.4 MB)                                                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 7.1s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   6.9s\n",
      "\u001b[2m => => # Downloading zope_interface-8.1.1-cp312-cp312-manylinux1_x86_64.manylin\n",
      "\u001b[0m\u001b[2m => => # ux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (264 kB) \n",
      "\u001b[0m\u001b[2m => => # Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)            \n",
      "\u001b[0m\u001b[2m => => # Downloading rsa-4.9.1-py3-none-any.whl (34 kB)                        \n",
      "\u001b[0m\u001b[2m => => # Downloading smmap-5.0.2-py3-none-any.whl (24 kB)                      \n",
      "\u001b[0m\u001b[2m => => # Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)                     \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 7.2s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   7.1s\n",
      "\u001b[2m => => # Downloading zope_interface-8.1.1-cp312-cp312-manylinux1_x86_64.manylin\n",
      "\u001b[0m\u001b[2m => => # ux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (264 kB) \n",
      "\u001b[0m\u001b[2m => => # Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)            \n",
      "\u001b[0m\u001b[2m => => # Downloading rsa-4.9.1-py3-none-any.whl (34 kB)                        \n",
      "\u001b[0m\u001b[2m => => # Downloading smmap-5.0.2-py3-none-any.whl (24 kB)                      \n",
      "\u001b[0m\u001b[2m => => # Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)                     \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 7.2s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   7.1s\n",
      "\u001b[2m => => # ux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (264 kB) \n",
      "\u001b[0m\u001b[2m => => # Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)            \n",
      "\u001b[0m\u001b[2m => => # Downloading rsa-4.9.1-py3-none-any.whl (34 kB)                        \n",
      "\u001b[0m\u001b[2m => => # Downloading smmap-5.0.2-py3-none-any.whl (24 kB)                      \n",
      "\u001b[0m\u001b[2m => => # Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)                     \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: antlr4-python3-runtime        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 7.4s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   7.2s\n",
      "\u001b[2m => => # Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)            \n",
      "\u001b[0m\u001b[2m => => # Downloading rsa-4.9.1-py3-none-any.whl (34 kB)                        \n",
      "\u001b[0m\u001b[2m => => # Downloading smmap-5.0.2-py3-none-any.whl (24 kB)                      \n",
      "\u001b[0m\u001b[2m => => # Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)                     \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: antlr4-python3-runtime        \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for antlr4-python3-runtime (setup.py): started       \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 7.5s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   7.4s\n",
      "\u001b[2m => => # Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)            \n",
      "\u001b[0m\u001b[2m => => # Downloading rsa-4.9.1-py3-none-any.whl (34 kB)                        \n",
      "\u001b[0m\u001b[2m => => # Downloading smmap-5.0.2-py3-none-any.whl (24 kB)                      \n",
      "\u001b[0m\u001b[2m => => # Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)                     \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: antlr4-python3-runtime        \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for antlr4-python3-runtime (setup.py): started       \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 7.7s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   7.5s\n",
      "\u001b[2m => => # Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)            \n",
      "\u001b[0m\u001b[2m => => # Downloading rsa-4.9.1-py3-none-any.whl (34 kB)                        \n",
      "\u001b[0m\u001b[2m => => # Downloading smmap-5.0.2-py3-none-any.whl (24 kB)                      \n",
      "\u001b[0m\u001b[2m => => # Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)                     \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: antlr4-python3-runtime        \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for antlr4-python3-runtime (setup.py): started       \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 7.7s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   7.5s\n",
      "\u001b[2m => => # Downloading smmap-5.0.2-py3-none-any.whl (24 kB)                      \n",
      "\u001b[0m\u001b[2m => => # Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)                     \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: antlr4-python3-runtime        \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for antlr4-python3-runtime (setup.py): started       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for antlr4-python3-runtime (setup.py): finished with \n",
      "\u001b[0m\u001b[2m => => # status 'done'                                                         \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 7.9s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   7.7s\n",
      "\u001b[2m => => #   Created wheel for antlr4-python3-runtime: filename=antlr4_python3_ru\n",
      "\u001b[0m\u001b[2m => => # ntime-4.9.3-py3-none-any.whl size=144591 sha256=df07ffde22a88b63d5994d\n",
      "\u001b[0m\u001b[2m => => # a7be7929ad661a700f677b80c0fbecc74866b76893                            \n",
      "\u001b[0m\u001b[2m => => #   Stored in directory: /tmp/pip-ephem-wheel-cache-5gxx_7cf/wheels/1f/b\n",
      "\u001b[0m\u001b[2m => => # e/48/13754633f1d08d1fbfc60d5e80ae1e5d7329500477685286cd               \n",
      "\u001b[0m\u001b[2m => => # Successfully built antlr4-python3-runtime                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 8.0s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   7.8s\n",
      "\u001b[2m => => #   Created wheel for antlr4-python3-runtime: filename=antlr4_python3_ru\n",
      "\u001b[0m\u001b[2m => => # ntime-4.9.3-py3-none-any.whl size=144591 sha256=df07ffde22a88b63d5994d\n",
      "\u001b[0m\u001b[2m => => # a7be7929ad661a700f677b80c0fbecc74866b76893                            \n",
      "\u001b[0m\u001b[2m => => #   Stored in directory: /tmp/pip-ephem-wheel-cache-5gxx_7cf/wheels/1f/b\n",
      "\u001b[0m\u001b[2m => => # e/48/13754633f1d08d1fbfc60d5e80ae1e5d7329500477685286cd               \n",
      "\u001b[0m\u001b[2m => => # Successfully built antlr4-python3-runtime                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 8.2s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   8.0s\n",
      "\u001b[2m => => #   Created wheel for antlr4-python3-runtime: filename=antlr4_python3_ru\n",
      "\u001b[0m\u001b[2m => => # ntime-4.9.3-py3-none-any.whl size=144591 sha256=df07ffde22a88b63d5994d\n",
      "\u001b[0m\u001b[2m => => # a7be7929ad661a700f677b80c0fbecc74866b76893                            \n",
      "\u001b[0m\u001b[2m => => #   Stored in directory: /tmp/pip-ephem-wheel-cache-5gxx_7cf/wheels/1f/b\n",
      "\u001b[0m\u001b[2m => => # e/48/13754633f1d08d1fbfc60d5e80ae1e5d7329500477685286cd               \n",
      "\u001b[0m\u001b[2m => => # Successfully built antlr4-python3-runtime                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 8.3s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   8.1s\n",
      "\u001b[2m => => #   Created wheel for antlr4-python3-runtime: filename=antlr4_python3_ru\n",
      "\u001b[0m\u001b[2m => => # ntime-4.9.3-py3-none-any.whl size=144591 sha256=df07ffde22a88b63d5994d\n",
      "\u001b[0m\u001b[2m => => # a7be7929ad661a700f677b80c0fbecc74866b76893                            \n",
      "\u001b[0m\u001b[2m => => #   Stored in directory: /tmp/pip-ephem-wheel-cache-5gxx_7cf/wheels/1f/b\n",
      "\u001b[0m\u001b[2m => => # e/48/13754633f1d08d1fbfc60d5e80ae1e5d7329500477685286cd               \n",
      "\u001b[0m\u001b[2m => => # Successfully built antlr4-python3-runtime                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 8.3s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   8.2s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 8.5s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   8.3s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 8.6s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   8.5s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 8.8s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   8.6s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 8.9s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   8.8s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 9.1s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   8.9s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 9.2s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   9.1s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 9.4s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   9.2s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 9.5s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   9.4s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 9.7s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   9.5s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 9.8s (7/8)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   9.7s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 10.0s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests   9.8s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 10.1s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  10.0s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 10.3s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  10.1s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 10.4s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  10.3s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 10.6s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  10.4s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 10.7s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  10.6s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 10.9s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  10.7s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 11.0s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  10.9s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 11.2s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  11.0s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 11.3s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  11.2s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 11.5s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  11.3s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 11.6s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  11.5s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 11.8s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  11.6s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 11.9s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  11.8s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 12.1s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  11.9s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 12.2s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  12.1s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 12.4s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  12.2s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 12.5s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  12.4s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 12.7s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  12.5s\n",
      "\u001b[2m => => #  graphql-relay, gitdb, gevent, Flask, docker, deepdiff, coloredlogs, c\n",
      "\u001b[0m\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 12.7s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  12.5s\n",
      "\u001b[2m => => # ffi, pynacl, opentelemetry-semantic-conventions, onnxruntime, graphene\n",
      "\u001b[0m\u001b[2m => => # , google-auth, gitpython, geventhttpclient, Flask-CORS, cryptography, \n",
      "\u001b[0m\u001b[2m => => # alembic, paramiko, opentelemetry-sdk, databricks-sdk, sagemaker-core, \n",
      "\u001b[0m\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: pyparsing                                     \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 12.9s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  12.7s\n",
      "\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: pyparsing                                     \n",
      "\u001b[0m\u001b[2m => => #     Found existing installation: pyparsing 2.4.7                      \n",
      "\u001b[0m\u001b[2m => => #     Uninstalling pyparsing-2.4.7:                                     \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled pyparsing-2.4.7                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 13.1s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  12.9s\n",
      "\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: pyparsing                                     \n",
      "\u001b[0m\u001b[2m => => #     Found existing installation: pyparsing 2.4.7                      \n",
      "\u001b[0m\u001b[2m => => #     Uninstalling pyparsing-2.4.7:                                     \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled pyparsing-2.4.7                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 13.2s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  13.0s\n",
      "\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: pyparsing                                     \n",
      "\u001b[0m\u001b[2m => => #     Found existing installation: pyparsing 2.4.7                      \n",
      "\u001b[0m\u001b[2m => => #     Uninstalling pyparsing-2.4.7:                                     \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled pyparsing-2.4.7                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 13.4s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  13.2s\n",
      "\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: pyparsing                                     \n",
      "\u001b[0m\u001b[2m => => #     Found existing installation: pyparsing 2.4.7                      \n",
      "\u001b[0m\u001b[2m => => #     Uninstalling pyparsing-2.4.7:                                     \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled pyparsing-2.4.7                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 13.5s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  13.3s\n",
      "\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: pyparsing                                     \n",
      "\u001b[0m\u001b[2m => => #     Found existing installation: pyparsing 2.4.7                      \n",
      "\u001b[0m\u001b[2m => => #     Uninstalling pyparsing-2.4.7:                                     \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled pyparsing-2.4.7                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 13.7s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  13.5s\n",
      "\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: pyparsing                                     \n",
      "\u001b[0m\u001b[2m => => #     Found existing installation: pyparsing 2.4.7                      \n",
      "\u001b[0m\u001b[2m => => #     Uninstalling pyparsing-2.4.7:                                     \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled pyparsing-2.4.7                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 13.8s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  13.6s\n",
      "\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: pyparsing                                     \n",
      "\u001b[0m\u001b[2m => => #     Found existing installation: pyparsing 2.4.7                      \n",
      "\u001b[0m\u001b[2m => => #     Uninstalling pyparsing-2.4.7:                                     \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled pyparsing-2.4.7                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 14.0s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  13.8s\n",
      "\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: pyparsing                                     \n",
      "\u001b[0m\u001b[2m => => #     Found existing installation: pyparsing 2.4.7                      \n",
      "\u001b[0m\u001b[2m => => #     Uninstalling pyparsing-2.4.7:                                     \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled pyparsing-2.4.7                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 14.1s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  13.9s\n",
      "\u001b[2m => => # mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serv\n",
      "\u001b[0m\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: pyparsing                                     \n",
      "\u001b[0m\u001b[2m => => #     Found existing installation: pyparsing 2.4.7                      \n",
      "\u001b[0m\u001b[2m => => #     Uninstalling pyparsing-2.4.7:                                     \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled pyparsing-2.4.7                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 14.2s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  14.0s\n",
      "\u001b[2m => => # e, sagemaker-mlops, sagemaker, sagemaker-ssh-helper                   \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: pyparsing                                     \n",
      "\u001b[0m\u001b[2m => => #     Found existing installation: pyparsing 2.4.7                      \n",
      "\u001b[0m\u001b[2m => => #     Uninstalling pyparsing-2.4.7:                                     \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled pyparsing-2.4.7                        \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: protobuf                                      \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 14.4s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  14.3s\n",
      "\u001b[2m => => #     Uninstalling pyparsing-2.4.7:                                     \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled pyparsing-2.4.7                        \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: protobuf                                      \n",
      "\u001b[0m\u001b[2m => => #     Found existing installation: protobuf 5.29.3                      \n",
      "\u001b[0m\u001b[2m => => #     Uninstalling protobuf-5.29.3:                                     \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled protobuf-5.29.3                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 14.6s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  14.4s\n",
      "\u001b[2m => => #     Uninstalling pyparsing-2.4.7:                                     \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled pyparsing-2.4.7                        \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: protobuf                                      \n",
      "\u001b[0m\u001b[2m => => #     Found existing installation: protobuf 5.29.3                      \n",
      "\u001b[0m\u001b[2m => => #     Uninstalling protobuf-5.29.3:                                     \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled protobuf-5.29.3                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 14.7s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  14.6s\n",
      "\u001b[2m => => #     Uninstalling pyparsing-2.4.7:                                     \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled pyparsing-2.4.7                        \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: protobuf                                      \n",
      "\u001b[0m\u001b[2m => => #     Found existing installation: protobuf 5.29.3                      \n",
      "\u001b[0m\u001b[2m => => #     Uninstalling protobuf-5.29.3:                                     \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled protobuf-5.29.3                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 14.9s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  14.7s\n",
      "\u001b[2m => => #     Uninstalling pyparsing-2.4.7:                                     \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled pyparsing-2.4.7                        \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: protobuf                                      \n",
      "\u001b[0m\u001b[2m => => #     Found existing installation: protobuf 5.29.3                      \n",
      "\u001b[0m\u001b[2m => => #     Uninstalling protobuf-5.29.3:                                     \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled protobuf-5.29.3                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 15.0s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  14.8s\n",
      "\u001b[2m => => #       Successfully uninstalled pyparsing-2.4.7                        \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: protobuf                                      \n",
      "\u001b[0m\u001b[2m => => #     Found existing installation: protobuf 5.29.3                      \n",
      "\u001b[0m\u001b[2m => => #     Uninstalling protobuf-5.29.3:                                     \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled protobuf-5.29.3                        \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: importlib-metadata                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 15.1s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  15.0s\n",
      "\u001b[2m => => #     Uninstalling protobuf-5.29.3:                                     \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled protobuf-5.29.3                        \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: importlib-metadata                            \n",
      "\u001b[0m\u001b[2m => => #     Found existing installation: importlib_metadata 8.6.1             \n",
      "\u001b[0m\u001b[2m => => #     Uninstalling importlib_metadata-8.6.1:                            \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled importlib_metadata-8.6.1               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 15.3s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  15.1s\n",
      "\u001b[2m => => #     Uninstalling protobuf-5.29.3:                                     \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled protobuf-5.29.3                        \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: importlib-metadata                            \n",
      "\u001b[0m\u001b[2m => => #     Found existing installation: importlib_metadata 8.6.1             \n",
      "\u001b[0m\u001b[2m => => #     Uninstalling importlib_metadata-8.6.1:                            \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled importlib_metadata-8.6.1               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 15.4s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  15.3s\n",
      "\u001b[2m => => #     Uninstalling protobuf-5.29.3:                                     \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled protobuf-5.29.3                        \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: importlib-metadata                            \n",
      "\u001b[0m\u001b[2m => => #     Found existing installation: importlib_metadata 8.6.1             \n",
      "\u001b[0m\u001b[2m => => #     Uninstalling importlib_metadata-8.6.1:                            \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled importlib_metadata-8.6.1               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 15.6s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  15.4s\n",
      "\u001b[2m => => #     Uninstalling protobuf-5.29.3:                                     \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled protobuf-5.29.3                        \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: importlib-metadata                            \n",
      "\u001b[0m\u001b[2m => => #     Found existing installation: importlib_metadata 8.6.1             \n",
      "\u001b[0m\u001b[2m => => #     Uninstalling importlib_metadata-8.6.1:                            \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled importlib_metadata-8.6.1               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 15.7s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  15.6s\n",
      "\u001b[2m => => #     Uninstalling protobuf-5.29.3:                                     \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled protobuf-5.29.3                        \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: importlib-metadata                            \n",
      "\u001b[0m\u001b[2m => => #     Found existing installation: importlib_metadata 8.6.1             \n",
      "\u001b[0m\u001b[2m => => #     Uninstalling importlib_metadata-8.6.1:                            \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled importlib_metadata-8.6.1               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 15.9s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  15.7s\n",
      "\u001b[2m => => #     Uninstalling protobuf-5.29.3:                                     \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled protobuf-5.29.3                        \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: importlib-metadata                            \n",
      "\u001b[0m\u001b[2m => => #     Found existing installation: importlib_metadata 8.6.1             \n",
      "\u001b[0m\u001b[2m => => #     Uninstalling importlib_metadata-8.6.1:                            \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled importlib_metadata-8.6.1               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 16.0s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  15.9s\n",
      "\u001b[2m => => #     Uninstalling protobuf-5.29.3:                                     \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled protobuf-5.29.3                        \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: importlib-metadata                            \n",
      "\u001b[0m\u001b[2m => => #     Found existing installation: importlib_metadata 8.6.1             \n",
      "\u001b[0m\u001b[2m => => #     Uninstalling importlib_metadata-8.6.1:                            \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled importlib_metadata-8.6.1               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 16.1s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  15.9s\n",
      "\u001b[2m => => #       Successfully uninstalled protobuf-5.29.3                        \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: importlib-metadata                            \n",
      "\u001b[0m\u001b[2m => => #     Found existing installation: importlib_metadata 8.6.1             \n",
      "\u001b[0m\u001b[2m => => #     Uninstalling importlib_metadata-8.6.1:                            \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled importlib_metadata-8.6.1               \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: blinker                                       \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 16.2s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  16.1s\n",
      "\u001b[2m => => #     Uninstalling importlib_metadata-8.6.1:                            \n",
      "\u001b[0m\u001b[2m => => #       Successfully uninstalled importlib_metadata-8.6.1               \n",
      "\u001b[0m\u001b[2m => => #   Attempting uninstall: blinker                                       \n",
      "\u001b[0m\u001b[2m => => #     Found existing installation: blinker 1.4                          \n",
      "\u001b[0m\u001b[2m => => # [notice] A new release of pip is available: 25.0.1 -> 25.3            \n",
      "\u001b[0m\u001b[2m => => # [notice] To update, run: python3 -m pip install --upgrade pip         \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 16.4s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  16.2s\n",
      "\u001b[2m => => # [notice] To update, run: python3 -m pip install --upgrade pip         \n",
      "\u001b[0m\u001b[2m => => # error: uninstall-distutils-installed-package                          \n",
      "\u001b[0m\u001b[2m => => #  Cannot uninstall blinker 1.4                                        \n",
      "\u001b[0m\u001b[2m => => # > It is a distutils installed project and thus we cannot accurately \n",
      "\u001b[0m\u001b[2m => => # determine which files belong to it which would lead to only a partial \n",
      "\u001b[0m\u001b[2m => => # uninstall.                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 16.5s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  16.4s\n",
      "\u001b[2m => => # [notice] To update, run: python3 -m pip install --upgrade pip         \n",
      "\u001b[0m\u001b[2m => => # error: uninstall-distutils-installed-package                          \n",
      "\u001b[0m\u001b[2m => => #  Cannot uninstall blinker 1.4                                        \n",
      "\u001b[0m\u001b[2m => => # > It is a distutils installed project and thus we cannot accurately \n",
      "\u001b[0m\u001b[2m => => # determine which files belong to it which would lead to only a partial \n",
      "\u001b[0m\u001b[2m => => # uninstall.                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 16.7s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  16.5s\n",
      "\u001b[2m => => # [notice] To update, run: python3 -m pip install --upgrade pip         \n",
      "\u001b[0m\u001b[2m => => # error: uninstall-distutils-installed-package                          \n",
      "\u001b[0m\u001b[2m => => #  Cannot uninstall blinker 1.4                                        \n",
      "\u001b[0m\u001b[2m => => # > It is a distutils installed project and thus we cannot accurately \n",
      "\u001b[0m\u001b[2m => => # determine which files belong to it which would lead to only a partial \n",
      "\u001b[0m\u001b[2m => => # uninstall.                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 16.8s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  16.7s\n",
      "\u001b[2m => => # [notice] To update, run: python3 -m pip install --upgrade pip         \n",
      "\u001b[0m\u001b[2m => => # error: uninstall-distutils-installed-package                          \n",
      "\u001b[0m\u001b[2m => => #  Cannot uninstall blinker 1.4                                        \n",
      "\u001b[0m\u001b[2m => => # > It is a distutils installed project and thus we cannot accurately \n",
      "\u001b[0m\u001b[2m => => # determine which files belong to it which would lead to only a partial \n",
      "\u001b[0m\u001b[2m => => # uninstall.                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 17.0s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  16.8s\n",
      "\u001b[2m => => # [notice] To update, run: python3 -m pip install --upgrade pip         \n",
      "\u001b[0m\u001b[2m => => # error: uninstall-distutils-installed-package                          \n",
      "\u001b[0m\u001b[2m => => #  Cannot uninstall blinker 1.4                                        \n",
      "\u001b[0m\u001b[2m => => # > It is a distutils installed project and thus we cannot accurately \n",
      "\u001b[0m\u001b[2m => => # determine which files belong to it which would lead to only a partial \n",
      "\u001b[0m\u001b[2m => => # uninstall.                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 17.1s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  17.0s\n",
      "\u001b[2m => => # [notice] To update, run: python3 -m pip install --upgrade pip         \n",
      "\u001b[0m\u001b[2m => => # error: uninstall-distutils-installed-package                          \n",
      "\u001b[0m\u001b[2m => => #  Cannot uninstall blinker 1.4                                        \n",
      "\u001b[0m\u001b[2m => => # > It is a distutils installed project and thus we cannot accurately \n",
      "\u001b[0m\u001b[2m => => # determine which files belong to it which would lead to only a partial \n",
      "\u001b[0m\u001b[2m => => # uninstall.                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 17.3s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  17.1s\n",
      "\u001b[2m => => # [notice] To update, run: python3 -m pip install --upgrade pip         \n",
      "\u001b[0m\u001b[2m => => # error: uninstall-distutils-installed-package                          \n",
      "\u001b[0m\u001b[2m => => #  Cannot uninstall blinker 1.4                                        \n",
      "\u001b[0m\u001b[2m => => # > It is a distutils installed project and thus we cannot accurately \n",
      "\u001b[0m\u001b[2m => => # determine which files belong to it which would lead to only a partial \n",
      "\u001b[0m\u001b[2m => => # uninstall.                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 17.4s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  17.3s\n",
      "\u001b[2m => => # [notice] To update, run: python3 -m pip install --upgrade pip         \n",
      "\u001b[0m\u001b[2m => => # error: uninstall-distutils-installed-package                          \n",
      "\u001b[0m\u001b[2m => => #  Cannot uninstall blinker 1.4                                        \n",
      "\u001b[0m\u001b[2m => => # > It is a distutils installed project and thus we cannot accurately \n",
      "\u001b[0m\u001b[2m => => # determine which files belong to it which would lead to only a partial \n",
      "\u001b[0m\u001b[2m => => # uninstall.                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 17.6s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  17.4s\n",
      "\u001b[2m => => # [notice] To update, run: python3 -m pip install --upgrade pip         \n",
      "\u001b[0m\u001b[2m => => # error: uninstall-distutils-installed-package                          \n",
      "\u001b[0m\u001b[2m => => #  Cannot uninstall blinker 1.4                                        \n",
      "\u001b[0m\u001b[2m => => # > It is a distutils installed project and thus we cannot accurately \n",
      "\u001b[0m\u001b[2m => => # determine which files belong to it which would lead to only a partial \n",
      "\u001b[0m\u001b[2m => => # uninstall.                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 17.7s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  17.6s\n",
      "\u001b[2m => => # [notice] To update, run: python3 -m pip install --upgrade pip         \n",
      "\u001b[0m\u001b[2m => => # error: uninstall-distutils-installed-package                          \n",
      "\u001b[0m\u001b[2m => => #  Cannot uninstall blinker 1.4                                        \n",
      "\u001b[0m\u001b[2m => => # > It is a distutils installed project and thus we cannot accurately \n",
      "\u001b[0m\u001b[2m => => # determine which files belong to it which would lead to only a partial \n",
      "\u001b[0m\u001b[2m => => # uninstall.                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 17.9s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  17.7s\n",
      "\u001b[2m => => # [notice] To update, run: python3 -m pip install --upgrade pip         \n",
      "\u001b[0m\u001b[2m => => # error: uninstall-distutils-installed-package                          \n",
      "\u001b[0m\u001b[2m => => #  Cannot uninstall blinker 1.4                                        \n",
      "\u001b[0m\u001b[2m => => # > It is a distutils installed project and thus we cannot accurately \n",
      "\u001b[0m\u001b[2m => => # determine which files belong to it which would lead to only a partial \n",
      "\u001b[0m\u001b[2m => => # uninstall.                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 18.0s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  17.9s\n",
      "\u001b[2m => => # [notice] To update, run: python3 -m pip install --upgrade pip         \n",
      "\u001b[0m\u001b[2m => => # error: uninstall-distutils-installed-package                          \n",
      "\u001b[0m\u001b[2m => => #  Cannot uninstall blinker 1.4                                        \n",
      "\u001b[0m\u001b[2m => => # > It is a distutils installed project and thus we cannot accurately \n",
      "\u001b[0m\u001b[2m => => # determine which files belong to it which would lead to only a partial \n",
      "\u001b[0m\u001b[2m => => # uninstall.                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 18.2s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  18.0s\n",
      "\u001b[2m => => # [notice] To update, run: python3 -m pip install --upgrade pip         \n",
      "\u001b[0m\u001b[2m => => # error: uninstall-distutils-installed-package                          \n",
      "\u001b[0m\u001b[2m => => #  Cannot uninstall blinker 1.4                                        \n",
      "\u001b[0m\u001b[2m => => # > It is a distutils installed project and thus we cannot accurately \n",
      "\u001b[0m\u001b[2m => => # determine which files belong to it which would lead to only a partial \n",
      "\u001b[0m\u001b[2m => => # uninstall.                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 18.3s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  18.2s\n",
      "\u001b[2m => => # [notice] To update, run: python3 -m pip install --upgrade pip         \n",
      "\u001b[0m\u001b[2m => => # error: uninstall-distutils-installed-package                          \n",
      "\u001b[0m\u001b[2m => => #  Cannot uninstall blinker 1.4                                        \n",
      "\u001b[0m\u001b[2m => => # > It is a distutils installed project and thus we cannot accurately \n",
      "\u001b[0m\u001b[2m => => # determine which files belong to it which would lead to only a partial \n",
      "\u001b[0m\u001b[2m => => # uninstall.                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 18.5s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  18.3s\n",
      "\u001b[2m => => # [notice] To update, run: python3 -m pip install --upgrade pip         \n",
      "\u001b[0m\u001b[2m => => # error: uninstall-distutils-installed-package                          \n",
      "\u001b[0m\u001b[2m => => #  Cannot uninstall blinker 1.4                                        \n",
      "\u001b[0m\u001b[2m => => # > It is a distutils installed project and thus we cannot accurately \n",
      "\u001b[0m\u001b[2m => => # determine which files belong to it which would lead to only a partial \n",
      "\u001b[0m\u001b[2m => => # uninstall.                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 18.6s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  18.5s\n",
      "\u001b[2m => => # [notice] To update, run: python3 -m pip install --upgrade pip         \n",
      "\u001b[0m\u001b[2m => => # error: uninstall-distutils-installed-package                          \n",
      "\u001b[0m\u001b[2m => => #  Cannot uninstall blinker 1.4                                        \n",
      "\u001b[0m\u001b[2m => => # > It is a distutils installed project and thus we cannot accurately \n",
      "\u001b[0m\u001b[2m => => # determine which files belong to it which would lead to only a partial \n",
      "\u001b[0m\u001b[2m => => # uninstall.                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 18.8s (7/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  18.6s\n",
      "\u001b[2m => => # [notice] To update, run: python3 -m pip install --upgrade pip         \n",
      "\u001b[0m\u001b[2m => => # error: uninstall-distutils-installed-package                          \n",
      "\u001b[0m\u001b[2m => => #  Cannot uninstall blinker 1.4                                        \n",
      "\u001b[0m\u001b[2m => => # > It is a distutils installed project and thus we cannot accurately \n",
      "\u001b[0m\u001b[2m => => # determine which files belong to it which would lead to only a partial \n",
      "\u001b[0m\u001b[2m => => # uninstall.                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 18.9s (8/8)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m\u001b[34m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  18.7s\n",
      "\u001b[0m                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "\u001b[6A\u001b[0G\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 19.0s (8/9)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m\u001b[34m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  18.7s\n",
      "\u001b[0m => exporting to image                                                     0.2s\n",
      " => => exporting layers                                                    0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 19.2s (8/9)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m\u001b[34m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  18.7s\n",
      "\u001b[0m => exporting to image                                                     0.3s\n",
      " => => exporting layers                                                    0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 19.3s (8/9)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m\u001b[34m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  18.7s\n",
      "\u001b[0m => exporting to image                                                     0.5s\n",
      " => => exporting layers                                                    0.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 19.5s (8/9)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m\u001b[34m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  18.7s\n",
      "\u001b[0m => exporting to image                                                     0.6s\n",
      " => => exporting layers                                                    0.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 19.6s (8/9)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m\u001b[34m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  18.7s\n",
      "\u001b[0m => exporting to image                                                     0.8s\n",
      " => => exporting layers                                                    0.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 19.8s (8/9)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m\u001b[34m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  18.7s\n",
      "\u001b[0m => exporting to image                                                     0.9s\n",
      " => => exporting layers                                                    0.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 19.9s (8/9)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m\u001b[34m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  18.7s\n",
      "\u001b[0m => exporting to image                                                     1.1s\n",
      " => => exporting layers                                                    1.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 20.1s (8/9)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m\u001b[34m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  18.7s\n",
      "\u001b[0m => exporting to image                                                     1.2s\n",
      "\u001b[34m => => exporting layers                                                    1.2s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 20.1s (9/9) FINISHED                                docker:default\n",
      "\u001b[34m => [internal] load build definition from dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 466B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/vllm/vllm-openai:v0.7.3         0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/vllm/vllm-openai:v0.7.3@sha256:4f4037303e8c7b694  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.81kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => [3/4] COPY app/ /app                                                   0.0s\n",
      "\u001b[0m\u001b[34m => [4/4] RUN python3 -m pip install s5cmd sagemaker-ssh-helper requests  18.7s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     1.2s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    1.2s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:bbc1d5c34ea7ccdfc8764d6f1a190ac69b1ce7ce8bb68  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/sagemaker_endpoint/vllm:v0.7.3                  0.0s\n",
      "\u001b[0m\u001b[?25hThe push refers to repository [596899493901.dkr.ecr.us-east-1.amazonaws.com/sagemaker_endpoint/vllm]\n",
      "\n",
      "\u001b[1B953a2e6f: Preparing \n",
      "\u001b[1B6e9d08a3: Preparing \n",
      "\u001b[1B6d3c77af: Preparing \n",
      "\u001b[1Ba7cb832c: Preparing \n",
      "\u001b[1Bc3283791: Preparing \n",
      "\u001b[1B0cefbfe9: Preparing \n",
      "\u001b[1B8fca8f78: Preparing \n",
      "\u001b[1Bb5c3ec02: Preparing \n",
      "\u001b[1B29888f25: Preparing \n",
      "\u001b[1Bbf18a086: Preparing \n",
      "\u001b[1B1edb23a0: Preparing \n",
      "\u001b[1Bdf1c3e22: Preparing \n",
      "\u001b[1Bb1d2cad1: Preparing \n",
      "\u001b[1B50a26d40: Preparing \n",
      "\u001b[1Bcc3a78e5: Preparing \n",
      "\u001b[1Bc7837807: Preparing \n",
      "\u001b[1B3d1076fd: Preparing \n",
      "\u001b[1B676771a0: Preparing \n",
      "\u001b[1B5100dff6: Preparing \n",
      "\u001b[1Beb1871b9: Preparing \n",
      "\u001b[1B3b7b997c: Preparing \n",
      "\u001b[1Bf58b80cd: Preparing \n",
      "\u001b[1Bbddcbe63: Preparing \n",
      "\u001b[1B23415d69: Preparing \n",
      "\u001b[1B33fba79b: Preparing \n",
      "\u001b[26B53a2e6f: Pushed   423.4MB/417.1MB0A\u001b[2K\u001b[18A\u001b[2K\u001b[15A\u001b[2K\u001b[12A\u001b[2K\u001b[11A\u001b[2K\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2Kv0.7.3: digest: sha256:bad4bd2008760c022591e3ba110792cd5617dac1ad062d4d9f069ab9cd211b02 size: 5779\n"
     ]
    }
   ],
   "source": [
    "cmd = f\"VLLM_REPO={VLLM_REPO} VLLM_VERSION={VLLM_VERSION} REPO_NAMESPACE={REPO_NAMESPACE} ACCOUNT={ACCOUNT} REGION={REGION} bash ./build_and_push.sh\"\n",
    "print(\"Runging:\", cmd)\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploy on SageMaker\n",
    "\n",
    "define the model and deploy on SageMaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.40.70)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.41.3-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.254.1)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-3.0.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.57.1)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.2-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: huggingface_hub in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.36.0)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-1.1.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: modelscope in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.31.0)\n",
      "Collecting modelscope\n",
      "  Downloading modelscope-1.32.0-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting botocore<1.42.0,>=1.41.3 (from boto3)\n",
      "  Downloading botocore-1.41.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Collecting s3transfer<0.16.0,>=0.15.0 (from boto3)\n",
      "  Downloading s3transfer-0.15.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.42.0,>=1.41.3->boto3) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.42.0,>=1.41.3->boto3) (1.26.19)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.42.0,>=1.41.3->boto3) (1.17.0)\n",
      "Collecting sagemaker-core<3.0.0,>=2.0.0 (from sagemaker)\n",
      "  Downloading sagemaker_core-2.0.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting sagemaker-train<2.0.0 (from sagemaker)\n",
      "  Downloading sagemaker_train-1.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting sagemaker-serve<2.0.0 (from sagemaker)\n",
      "  Downloading sagemaker_serve-1.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting sagemaker-mlops<2.0.0 (from sagemaker)\n",
      "  Downloading sagemaker_mlops-1.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker-core<3.0.0,>=2.0.0->sagemaker) (2.12.3)\n",
      "Requirement already satisfied: PyYAML<7.0,>=6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker-core<3.0.0,>=2.0.0->sagemaker) (6.0.3)\n",
      "Requirement already satisfied: jsonschema<5.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker-core<3.0.0,>=2.0.0->sagemaker) (4.25.1)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=4.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker-core<3.0.0,>=2.0.0->sagemaker) (4.5.0)\n",
      "Collecting rich<14.0.0,>=13.0.0 (from sagemaker-core<3.0.0,>=2.0.0->sagemaker)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker-core<3.0.0,>=2.0.0->sagemaker) (4.0.3)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker-core<3.0.0,>=2.0.0->sagemaker) (6.11.0)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker-core<3.0.0,>=2.0.0->sagemaker) (4.15.0)\n",
      "Requirement already satisfied: pytz>=2021.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker-core<3.0.0,>=2.0.0->sagemaker) (2025.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker-core<3.0.0,>=2.0.0->sagemaker) (2.32.5)\n",
      "Requirement already satisfied: attrs>=20.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker-core<3.0.0,>=2.0.0->sagemaker) (25.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker-core<3.0.0,>=2.0.0->sagemaker) (24.2)\n",
      "Collecting protobuf<5.0,>=3.12 (from sagemaker-core<3.0.0,>=2.0.0->sagemaker)\n",
      "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker-core<3.0.0,>=2.0.0->sagemaker) (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker-core<3.0.0,>=2.0.0->sagemaker) (1.26.4)\n",
      "Requirement already satisfied: smdebug_rulesconfig>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker-core<3.0.0,>=2.0.0->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: schema>=0.7.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker-core<3.0.0,>=2.0.0->sagemaker) (0.7.8)\n",
      "Requirement already satisfied: omegaconf>=2.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker-core<3.0.0,>=2.0.0->sagemaker) (2.3.0)\n",
      "Requirement already satisfied: torch>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker-core<3.0.0,>=2.0.0->sagemaker) (2.6.0+cu124)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker-core<3.0.0,>=2.0.0->sagemaker) (1.15.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (3.23.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema<5.0.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema<5.0.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema<5.0.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (0.27.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.20.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.20.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.20.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (2025.10.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (2.19.2)\n",
      "Collecting deepdiff (from sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading deepdiff-8.6.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting mlflow (from sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading mlflow-3.6.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting sagemaker_schema_inference_artifacts (from sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading sagemaker_schema_inference_artifacts-0.0.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pytest (from sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading pytest-9.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker-serve<2.0.0->sagemaker) (4.67.1)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker-serve<2.0.0->sagemaker) (7.1.0)\n",
      "Collecting tritonclient[http] (from sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading tritonclient-2.62.0-py3-none-manylinux1_x86_64.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: onnx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker-serve<2.0.0->sagemaker) (1.19.1)\n",
      "Collecting onnxruntime (from sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: graphene<4,>=3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker-train<2.0.0->sagemaker) (3.4.3)\n",
      "Requirement already satisfied: tblib>=1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker-train<2.0.0->sagemaker) (3.2.0)\n",
      "Collecting paramiko>=2.11.0 (from sagemaker-train<2.0.0->sagemaker)\n",
      "  Downloading paramiko-4.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from graphene<4,>=3->sagemaker-train<2.0.0->sagemaker) (3.2.6)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from graphene<4,>=3->sagemaker-train<2.0.0->sagemaker) (3.2.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2025.10.23)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from modelscope) (80.9.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.0.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (0.1.2)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from omegaconf>=2.1.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (4.9.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=1.0.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (2025.2)\n",
      "Collecting bcrypt>=3.2 (from paramiko>=2.11.0->sagemaker-train<2.0.0->sagemaker)\n",
      "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: cryptography>=3.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from paramiko>=2.11.0->sagemaker-train<2.0.0->sagemaker) (46.0.3)\n",
      "Collecting invoke>=2.0 (from paramiko>=2.11.0->sagemaker-train<2.0.0->sagemaker)\n",
      "  Downloading invoke-2.2.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pynacl>=1.5 (from paramiko>=2.11.0->sagemaker-train<2.0.0->sagemaker)\n",
      "  Downloading pynacl-1.6.1-cp38-abi3-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: cffi>=1.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cryptography>=3.3->paramiko>=2.11.0->sagemaker-train<2.0.0->sagemaker) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cffi>=1.14->cryptography>=3.3->paramiko>=2.11.0->sagemaker-train<2.0.0->sagemaker) (2.22)\n",
      "Collecting cffi>=1.14 (from cryptography>=3.3->paramiko>=2.11.0->sagemaker-train<2.0.0->sagemaker)\n",
      "  Using cached cffi-2.0.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.9.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (1.3.0)\n",
      "Collecting orderly-set<6,>=5.4.1 (from deepdiff->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading orderly_set-5.5.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->sagemaker-core<3.0.0,>=2.0.0->sagemaker) (3.0.3)\n",
      "Collecting mlflow-skinny==3.6.0 (from mlflow->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading mlflow_skinny-3.6.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting mlflow-tracing==3.6.0 (from mlflow->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading mlflow_tracing-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting Flask-CORS<7 (from mlflow->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading flask_cors-6.0.1-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: Flask<4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from mlflow->sagemaker-serve<2.0.0->sagemaker) (3.1.2)\n",
      "Collecting alembic!=1.10.0,<2 (from mlflow->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading alembic-1.17.2-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from mlflow->sagemaker-serve<2.0.0->sagemaker) (7.1.0)\n",
      "Collecting gunicorn<24 (from mlflow->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting huey<3,>=2.5.0 (from mlflow->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading huey-2.5.4-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: matplotlib<4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from mlflow->sagemaker-serve<2.0.0->sagemaker) (3.10.7)\n",
      "Requirement already satisfied: pyarrow<23,>=4.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from mlflow->sagemaker-serve<2.0.0->sagemaker) (20.0.0)\n",
      "Requirement already satisfied: scikit-learn<2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from mlflow->sagemaker-serve<2.0.0->sagemaker) (1.7.2)\n",
      "Collecting sqlalchemy<3,>=1.4.0 (from mlflow->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading sqlalchemy-2.0.44-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting cachetools<7,>=5.0.0 (from mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading cachetools-6.2.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.0->sagemaker) (8.3.0)\n",
      "Requirement already satisfied: cloudpickle<4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.0->sagemaker) (3.1.1)\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading databricks_sdk-0.73.0-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: fastapi<1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.0->sagemaker) (0.119.1)\n",
      "Collecting gitpython<4,>=3.1.9 (from mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Using cached gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-proto<3,>=1.9.0 (from mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading opentelemetry_proto-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting python-dotenv<2,>=0.19.0 (from mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting sqlparse<1,>=0.4.0 (from mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading sqlparse-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: uvicorn<1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.0->sagemaker) (0.38.0)\n",
      "Collecting Mako (from alembic!=1.10.0,<2->mlflow->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: tomli in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow->sagemaker-serve<2.0.0->sagemaker) (2.3.0)\n",
      "Collecting google-auth~=2.0 (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading google_auth-2.43.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from fastapi<1->mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.0->sagemaker) (0.48.0)\n",
      "Requirement already satisfied: blinker>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from Flask<4->mlflow->sagemaker-serve<2.0.0->sagemaker) (1.9.0)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from Flask<4->mlflow->sagemaker-serve<2.0.0->sagemaker) (2.2.0)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from Flask<4->mlflow->sagemaker-serve<2.0.0->sagemaker) (3.1.3)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.0->sagemaker) (4.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib<4->mlflow->sagemaker-serve<2.0.0->sagemaker) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib<4->mlflow->sagemaker-serve<2.0.0->sagemaker) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib<4->mlflow->sagemaker-serve<2.0.0->sagemaker) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib<4->mlflow->sagemaker-serve<2.0.0->sagemaker) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib<4->mlflow->sagemaker-serve<2.0.0->sagemaker) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib<4->mlflow->sagemaker-serve<2.0.0->sagemaker) (3.2.5)\n",
      "INFO: pip is looking at multiple versions of opentelemetry-proto to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opentelemetry-proto<3,>=1.9.0 (from mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading opentelemetry_proto-1.37.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading opentelemetry_proto-1.35.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading opentelemetry_proto-1.34.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading opentelemetry_proto-1.33.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading opentelemetry_proto-1.33.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "INFO: pip is still looking at multiple versions of opentelemetry-proto to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading opentelemetry_proto-1.32.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading opentelemetry_proto-1.32.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading opentelemetry_proto-1.31.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading opentelemetry_proto-1.31.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading opentelemetry_proto-1.30.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading opentelemetry_proto-1.28.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading opentelemetry_proto-1.28.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading opentelemetry_proto-1.28.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading opentelemetry_proto-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rsa<5,>=3.1.4->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.0->sagemaker) (0.6.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn<2->mlflow->sagemaker-serve<2.0.0->sagemaker) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn<2->mlflow->sagemaker-serve<2.0.0->sagemaker) (3.6.0)\n",
      "Collecting greenlet>=1 (from sqlalchemy<3,>=1.4.0->mlflow->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading greenlet-3.2.4-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from starlette<0.49.0,>=0.40.0->fastapi<1->mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.0->sagemaker) (4.11.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.49.0,>=0.40.0->fastapi<1->mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.0->sagemaker) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.49.0,>=0.40.0->fastapi<1->mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.0->sagemaker) (1.3.1)\n",
      "Requirement already satisfied: h11>=0.8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from uvicorn<1->mlflow-skinny==3.6.0->mlflow->sagemaker-serve<2.0.0->sagemaker) (0.16.0)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from onnx->sagemaker-serve<2.0.0->sagemaker) (0.5.1)\n",
      "Collecting coloredlogs (from onnxruntime->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting iniconfig>=1.0.1 (from pytest->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading iniconfig-2.3.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting pluggy<2,>=1.5 (from pytest->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Using cached pluggy-1.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: json5>=0.9.22 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker_schema_inference_artifacts->sagemaker-serve<2.0.0->sagemaker) (0.12.1)\n",
      "Collecting perf-analyzer (from tritonclient[http]->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading perf_analyzer-0.1.0-py3-none-any.whl.metadata (135 bytes)\n",
      "Collecting python-rapidjson>=0.9.1 (from tritonclient[http]->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading python_rapidjson-1.22-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (24 kB)\n",
      "Collecting urllib3!=2.2.0,<3,>=1.25.4 (from botocore<1.42.0,>=1.41.3->boto3)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.1 (from tritonclient[http]->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading aiohttp-3.13.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting geventhttpclient>=2.3.3 (from tritonclient[http]->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading geventhttpclient-2.3.5-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.1->tritonclient[http]->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.1->tritonclient[http]->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp<4.0.0,>=3.8.1->tritonclient[http]->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.1->tritonclient[http]->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading frozenlist-1.8.0-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.1->tritonclient[http]->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading multidict-6.7.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.1->tritonclient[http]->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading propcache-0.4.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.1->tritonclient[http]->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading yarl-1.22.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Collecting gevent (from geventhttpclient>=2.3.3->tritonclient[http]->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading gevent-25.9.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: brotli in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from geventhttpclient>=2.3.3->tritonclient[http]->sagemaker-serve<2.0.0->sagemaker) (1.1.0)\n",
      "Collecting zope.event (from gevent->geventhttpclient>=2.3.3->tritonclient[http]->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading zope_event-6.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting zope.interface (from gevent->geventhttpclient>=2.3.3->tritonclient[http]->sagemaker-serve<2.0.0->sagemaker)\n",
      "  Downloading zope_interface-8.1.1-cp310-cp310-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (45 kB)\n",
      "Downloading boto3-1.41.3-py3-none-any.whl (139 kB)\n",
      "Downloading botocore-1.41.3-py3-none-any.whl (14.3 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m14.3/14.3 MB\u001b[0m \u001b[31m163.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading s3transfer-0.15.0-py3-none-any.whl (85 kB)\n",
      "Downloading sagemaker-3.0.1-py3-none-any.whl (9.5 kB)\n",
      "Downloading sagemaker_core-2.0.1-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading sagemaker_mlops-1.0-py3-none-any.whl (102 kB)\n",
      "Downloading sagemaker_serve-1.0-py3-none-any.whl (196 kB)\n",
      "Downloading sagemaker_train-1.0-py3-none-any.whl (121 kB)\n",
      "Downloading transformers-4.57.2-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m137.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading modelscope-1.32.0-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m152.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading paramiko-4.0.0-py3-none-any.whl (223 kB)\n",
      "Downloading bcrypt-5.0.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (278 kB)\n",
      "Downloading invoke-2.2.1-py3-none-any.whl (160 kB)\n",
      "Downloading pynacl-1.6.1-cp38-abi3-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m129.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached cffi-2.0.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216 kB)\n",
      "Downloading deepdiff-8.6.1-py3-none-any.whl (91 kB)\n",
      "Downloading orderly_set-5.5.0-py3-none-any.whl (13 kB)\n",
      "Downloading mlflow-3.6.0-py3-none-any.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m188.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mlflow_skinny-3.6.0-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m174.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mlflow_tracing-3.6.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.17.2-py3-none-any.whl (248 kB)\n",
      "Downloading cachetools-6.2.2-py3-none-any.whl (11 kB)\n",
      "Downloading databricks_sdk-0.73.0-py3-none-any.whl (753 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m753.9/753.9 kB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading flask_cors-6.0.1-py3-none-any.whl (13 kB)\n",
      "Using cached gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading google_auth-2.43.0-py2.py3-none-any.whl (223 kB)\n",
      "Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "Downloading huey-2.5.4-py3-none-any.whl (76 kB)\n",
      "Downloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\n",
      "Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading sqlalchemy-2.0.44-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m165.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sqlparse-0.5.3-py3-none-any.whl (44 kB)\n",
      "Downloading greenlet-3.2.4-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (584 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m584.4/584.4 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m184.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading pytest-9.0.1-py3-none-any.whl (373 kB)\n",
      "Using cached pluggy-1.6.0-py3-none-any.whl (20 kB)\n",
      "Downloading iniconfig-2.3.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading sagemaker_schema_inference_artifacts-0.0.5-py3-none-any.whl (621 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m621.9/621.9 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tritonclient-2.62.0-py3-none-manylinux1_x86_64.whl (111 kB)\n",
      "Downloading aiohttp-3.13.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m121.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading multidict-6.7.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading yarl-1.22.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (346 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.8.0-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (219 kB)\n",
      "Downloading geventhttpclient-2.3.5-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (114 kB)\n",
      "Downloading propcache-0.4.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (196 kB)\n",
      "Downloading python_rapidjson-1.22-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m119.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gevent-25.9.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m136.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading perf_analyzer-0.1.0-py3-none-any.whl (2.3 kB)\n",
      "Downloading zope_event-6.1-py3-none-any.whl (6.4 kB)\n",
      "Downloading zope_interface-8.1.1-cp310-cp310-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (254 kB)\n",
      "Installing collected packages: perf-analyzer, huey, flatbuffers, zope.interface, zope.event, urllib3, sqlparse, smmap, sagemaker_schema_inference_artifacts, python-rapidjson, python-dotenv, pyasn1-modules, protobuf, propcache, pluggy, orderly-set, multidict, Mako, invoke, iniconfig, humanfriendly, gunicorn, greenlet, frozenlist, cffi, cachetools, bcrypt, async-timeout, aiohappyeyeballs, yarl, tritonclient, sqlalchemy, rich, pytest, pynacl, opentelemetry-proto, opentelemetry-api, google-auth, gitdb, gevent, deepdiff, coloredlogs, botocore, aiosignal, s3transfer, paramiko, opentelemetry-semantic-conventions, onnxruntime, modelscope, gitpython, geventhttpclient, Flask-CORS, databricks-sdk, alembic, aiohttp, opentelemetry-sdk, boto3, transformers, sagemaker-core, mlflow-tracing, mlflow-skinny, sagemaker-train, mlflow, sagemaker-serve, sagemaker-mlops, sagemaker\n",
      "\u001b[2K  Attempting uninstall: urllib3\u001b[0m \u001b[32m 3/66\u001b[0m [zope.interface]\n",
      "\u001b[2K    Found existing installation: urllib3 1.26.19\u001b[0m \u001b[32m 3/66\u001b[0m [zope.interface]\n",
      "\u001b[2K    Uninstalling urllib3-1.26.19:\u001b[0m \u001b[32m 3/66\u001b[0m [zope.interface]\n",
      "\u001b[2K      Successfully uninstalled urllib3-1.26.19\u001b[0m \u001b[32m 3/66\u001b[0m [zope.interface]\n",
      "\u001b[2K  Attempting uninstall: protobufm\u001b[0m \u001b[32m11/66\u001b[0m [pyasn1-modules]\n",
      "\u001b[2K    Found existing installation: protobuf 6.31.1\u001b[0m \u001b[32m11/66\u001b[0m [pyasn1-modules]\n",
      "\u001b[2K    Uninstalling protobuf-6.31.1:\u001b[0m \u001b[32m11/66\u001b[0m [pyasn1-modules]\n",
      "\u001b[2K      Successfully uninstalled protobuf-6.31.1\u001b[0m \u001b[32m11/66\u001b[0m [pyasn1-modules]\n",
      "\u001b[2K  Attempting uninstall: cffi\u001b[0m\u001b[90m\u001b[0m \u001b[32m19/66\u001b[0m [iniconfig]]\n",
      "\u001b[2K    Found existing installation: cffi 1.17.1\u001b[0m \u001b[32m19/66\u001b[0m [iniconfig]\n",
      "\u001b[2K    Uninstalling cffi-1.17.1:\u001b[90m\u001b[0m \u001b[32m19/66\u001b[0m [iniconfig]\n",
      "\u001b[2K      Successfully uninstalled cffi-1.17.1\u001b[0m \u001b[32m24/66\u001b[0m [cffi]]\n",
      "\u001b[2K  Attempting uninstall: rich0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m31/66\u001b[0m [sqlalchemy]\n",
      "\u001b[2K    Found existing installation: rich 14.2.0\u001b[0m \u001b[32m31/66\u001b[0m [sqlalchemy]\n",
      "\u001b[2K    Uninstalling rich-14.2.0:1m\u001b[0m\u001b[90m\u001b[0m \u001b[32m31/66\u001b[0m [sqlalchemy]\n",
      "\u001b[2K      Successfully uninstalled rich-14.2.0\u001b[0m \u001b[32m31/66\u001b[0m [sqlalchemy]\n",
      "\u001b[2K  Attempting uninstall: botocore\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m41/66\u001b[0m [coloredlogs]\n",
      "\u001b[2K    Found existing installation: botocore 1.40.70\u001b[0m \u001b[32m41/66\u001b[0m [coloredlogs]\n",
      "\u001b[2K    Uninstalling botocore-1.40.70:91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m41/66\u001b[0m [coloredlogs]\n",
      "\u001b[2K      Successfully uninstalled botocore-1.40.70\u001b[0m \u001b[32m41/66\u001b[0m [coloredlogs]\n",
      "\u001b[2K  Attempting uninstall: s3transfer\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m43/66\u001b[0m [aiosignal]\n",
      "\u001b[2K    Found existing installation: s3transfer 0.14.0\u001b[0m \u001b[32m43/66\u001b[0m [aiosignal]\n",
      "\u001b[2K    Uninstalling s3transfer-0.14.0:[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m43/66\u001b[0m [aiosignal]\n",
      "\u001b[2K      Successfully uninstalled s3transfer-0.14.0m\u001b[0m \u001b[32m43/66\u001b[0m [aiosignal]\n",
      "\u001b[2K  Attempting uninstall: modelscope\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m47/66\u001b[0m [onnxruntime]ntic-conventions]\n",
      "\u001b[2K    Found existing installation: modelscope 1.31.0m\u001b[0m \u001b[32m47/66\u001b[0m [onnxruntime]\n",
      "\u001b[2K    Uninstalling modelscope-1.31.0:\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m48/66\u001b[0m [modelscope]\n",
      "\u001b[2K      Successfully uninstalled modelscope-1.31.0[0m\u001b[90m\u001b[0m \u001b[32m48/66\u001b[0m [modelscope]\n",
      "\u001b[2K  Attempting uninstall: boto3\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m54/66\u001b[0m [aiohttp]ks-sdk]t]\n",
      "\u001b[2K    Found existing installation: boto3 1.40.70\u001b[0m\u001b[90m\u001b[0m \u001b[32m54/66\u001b[0m [aiohttp]\n",
      "\u001b[2K    Uninstalling boto3-1.40.70:\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m54/66\u001b[0m [aiohttp]\n",
      "\u001b[2K      Successfully uninstalled boto3-1.40.70m\u001b[0m\u001b[90m\u001b[0m \u001b[32m54/66\u001b[0m [aiohttp]\n",
      "\u001b[2K  Attempting uninstall: transformers\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m56/66\u001b[0m [boto3]\n",
      "\u001b[2K    Found existing installation: transformers 4.57.1[90m\u001b[0m \u001b[32m56/66\u001b[0m [boto3]\n",
      "\u001b[2K    Uninstalling transformers-4.57.1:\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m57/66\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.57.10m\u001b[90m\u001b[0m \u001b[32m57/66\u001b[0m [transformers]\n",
      "\u001b[2K  Attempting uninstall: sagemaker-core\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m57/66\u001b[0m [transformers]\n",
      "\u001b[2K    Found existing installation: sagemaker-core 1.0.5990m\u001b[0m \u001b[32m57/66\u001b[0m [transformers]\n",
      "\u001b[2K    Uninstalling sagemaker-core-1.0.59:[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m57/66\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled sagemaker-core-1.0.59\u001b[90m\u001b[0m \u001b[32m57/66\u001b[0m [transformers]\n",
      "\u001b[2K  Attempting uninstall: sagemaker\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m63/66\u001b[0m [sagemaker-serve]\n",
      "\u001b[2K    Found existing installation: sagemaker 2.254.1m\u001b[0m\u001b[90m\u001b[0m \u001b[32m63/66\u001b[0m [sagemaker-serve]\n",
      "\u001b[2K    Uninstalling sagemaker-2.254.1:\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m63/66\u001b[0m [sagemaker-serve]\n",
      "\u001b[2K      Successfully uninstalled sagemaker-2.254.1[0m\u001b[90m\u001b[0m \u001b[32m65/66\u001b[0m [sagemaker]aker-serve]\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m66/66\u001b[0m [sagemaker]aker]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.42.59 requires botocore==1.40.59, but you have botocore 1.41.3 which is incompatible.\n",
      "awscli 1.42.59 requires s3transfer<0.15.0,>=0.14.0, but you have s3transfer 0.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Flask-CORS-6.0.1 Mako-1.3.10 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 alembic-1.17.2 async-timeout-5.0.1 bcrypt-5.0.0 boto3-1.41.3 botocore-1.41.3 cachetools-6.2.2 cffi-2.0.0 coloredlogs-15.0.1 databricks-sdk-0.73.0 deepdiff-8.6.1 flatbuffers-25.9.23 frozenlist-1.8.0 gevent-25.9.1 geventhttpclient-2.3.5 gitdb-4.0.12 gitpython-3.1.45 google-auth-2.43.0 greenlet-3.2.4 gunicorn-23.0.0 huey-2.5.4 humanfriendly-10.0 iniconfig-2.3.0 invoke-2.2.1 mlflow-3.6.0 mlflow-skinny-3.6.0 mlflow-tracing-3.6.0 modelscope-1.32.0 multidict-6.7.0 onnxruntime-1.16.3 opentelemetry-api-1.38.0 opentelemetry-proto-1.27.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 orderly-set-5.5.0 paramiko-4.0.0 perf-analyzer-0.1.0 pluggy-1.6.0 propcache-0.4.1 protobuf-4.25.8 pyasn1-modules-0.4.2 pynacl-1.6.1 pytest-9.0.1 python-dotenv-1.2.1 python-rapidjson-1.22 rich-13.9.4 s3transfer-0.15.0 sagemaker-3.0.1 sagemaker-core-2.0.1 sagemaker-mlops-1.0 sagemaker-serve-1.0 sagemaker-train-1.0 sagemaker_schema_inference_artifacts-0.0.5 smmap-5.0.2 sqlalchemy-2.0.44 sqlparse-0.5.3 transformers-4.57.2 tritonclient-2.62.0 urllib3-2.5.0 yarl-1.22.0 zope.event-6.1 zope.interface-8.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U boto3 sagemaker transformers huggingface_hub modelscope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 Init SageMaker session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sess.default_bucket()\n",
    "\n",
    "sagemaker_client = boto3.client(\"sagemaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Download and upload model file\n",
    "\n",
    "Firstly, you need to prepare model weights and upload to S3. You can download from HuggingFace, ModelScope or upload your own model. \n",
    "\n",
    "If you want vllm to automatically pull the model when it starts, this step can be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_model_path: /home/ec2-user/SageMaker/models/Qwen-QwQ-32B\n"
     ]
    }
   ],
   "source": [
    "model_name = MODEL_ID.replace(\"/\", \"-\").replace(\".\", \"-\")\n",
    "local_model_path = os.environ['HOME'] + \"/SageMaker/models/\" + model_name\n",
    "cache_model_path = os.environ['HOME'] + \"/SageMaker/huggingface_cache\"\n",
    "s3_model_path = f\"s3://{default_bucket}/models/\" + model_name\n",
    "\n",
    "%mkdir -p code {local_model_path}\n",
    "\n",
    "print(\"local_model_path:\", local_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Option 1: Global region (download from HuggingFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Fetching 27 files: 100%|| 27/27 [00:00<00:00, 1403.93it/s]\n",
      "/home/ec2-user/SageMaker/models/Qwen-QwQ-32B\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli download --resume-download {MODEL_ID} --local-dir {local_model_path} --cache-dir {cache_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Option 2: China region  (download from ModelScope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!modelscope download --local_dir {local_model_path} {MODEL_ID} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### upload to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../../../models/Qwen-QwQ-32B/.cache/huggingface/download/.gitattributes.lock to s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B/.cache/huggingface/download/.gitattributes.lock\n",
      "upload: ../../../models/Qwen-QwQ-32B/.cache/huggingface/download/model-00001-of-00014.safetensors.lock to s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B/.cache/huggingface/download/model-00001-of-00014.safetensors.lock\n",
      "upload: ../../../models/Qwen-QwQ-32B/.cache/huggingface/download/model-00004-of-00014.safetensors.lock to s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B/.cache/huggingface/download/model-00004-of-00014.safetensors.lock\n",
      "upload: ../../../models/Qwen-QwQ-32B/.cache/huggingface/download/config.json.lock to s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B/.cache/huggingface/download/config.json.lock\n",
      "upload: ../../../models/Qwen-QwQ-32B/.cache/huggingface/download/LICENSE.lock to s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B/.cache/huggingface/download/LICENSE.lock\n",
      "upload: ../../../models/Qwen-QwQ-32B/.cache/huggingface/download/README.md.lock to s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B/.cache/huggingface/download/README.md.lock\n",
      "upload: ../../../models/Qwen-QwQ-32B/.cache/huggingface/download/figures/benchmark.jpg.lock to s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B/.cache/huggingface/download/figures/benchmark.jpg.lock\n",
      "upload: ../../../models/Qwen-QwQ-32B/.cache/huggingface/download/added_tokens.json.lock to s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B/.cache/huggingface/download/added_tokens.json.lock\n",
      "upload: ../../../models/Qwen-QwQ-32B/.cache/huggingface/download/merges.txt.lock to s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B/.cache/huggingface/download/merges.txt.lock\n",
      "upload: ../../../models/Qwen-QwQ-32B/.cache/huggingface/download/model-00005-of-00014.safetensors.lock to s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B/.cache/huggingface/download/model-00005-of-00014.safetensors.lock\n",
      "upload: ../../../models/Qwen-QwQ-32B/.cache/huggingface/download/model-00002-of-00014.safetensors.lock to s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B/.cache/huggingface/download/model-00002-of-00014.safetensors.lock\n",
      "upload: ../../../models/Qwen-QwQ-32B/.cache/huggingface/download/model-00006-of-00014.safetensors.lock to s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B/.cache/huggingface/download/model-00006-of-00014.safetensors.lock\n",
      "upload: ../../../models/Qwen-QwQ-32B/.cache/huggingface/download/model-00007-of-00014.safetensors.lock to s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B/.cache/huggingface/download/model-00007-of-00014.safetensors.lock\n",
      "upload: ../../../models/Qwen-QwQ-32B/.cache/huggingface/download/model-00008-of-00014.safetensors.lock to s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B/.cache/huggingface/download/model-00008-of-00014.safetensors.lock\n",
      "upload: ../../../models/Qwen-QwQ-32B/.cache/huggingface/download/model-00010-of-00014.safetensors.lock to s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B/.cache/huggingface/download/model-00010-of-00014.safetensors.lock\n",
      "upload: ../../../models/Qwen-QwQ-32B/.cache/huggingface/download/model-00009-of-00014.safetensors.lock to s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B/.cache/huggingface/download/model-00009-of-00014.safetensors.lock\n",
      "upload: ../../../models/Qwen-QwQ-32B/.cache/huggingface/download/model-00011-of-00014.safetensors.lock to s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B/.cache/huggingface/download/model-00011-of-00014.safetensors.lock\n",
      "upload: ../../../models/Qwen-QwQ-32B/.cache/huggingface/download/model-00013-of-00014.safetensors.lock to s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B/.cache/huggingface/download/model-00013-of-00014.safetensors.lock\n",
      "upload: ../../../models/Qwen-QwQ-32B/.cache/huggingface/download/model-00014-of-00014.safetensors.lock to s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B/.cache/huggingface/download/model-00014-of-00014.safetensors.lock\n",
      "upload: ../../../models/Qwen-QwQ-32B/.cache/huggingface/download/model-00012-of-00014.safetensors.lock to s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B/.cache/huggingface/download/model-00012-of-00014.safetensors.lock\n",
      "upload: ../../../models/Qwen-QwQ-32B/.cache/huggingface/download/model.safetensors.index.json.lock to s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B/.cache/huggingface/download/model.safetensors.index.json.lock\n",
      "upload: ../../../models/Qwen-QwQ-32B/.cache/huggingface/download/model-00003-of-00014.safetensors.lock to s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B/.cache/huggingface/download/model-00003-of-00014.safetensors.lock\n",
      "upload: ../../../models/Qwen-QwQ-32B/.cache/huggingface/download/generation_config.json.lock to s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B/.cache/huggingface/download/generation_config.json.lock\n",
      "upload: ../../../models/Qwen-QwQ-32B/.cache/huggingface/download/special_tokens_map.json.lock to s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B/.cache/huggingface/download/special_tokens_map.json.lock\n",
      "upload: ../../../models/Qwen-QwQ-32B/.cache/huggingface/download/tokenizer.json.lock to s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B/.cache/huggingface/download/tokenizer.json.lock\n",
      "upload: ../../../models/Qwen-QwQ-32B/.cache/huggingface/download/tokenizer_config.json.lock to s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B/.cache/huggingface/download/tokenizer_config.json.lock\n",
      "upload: ../../../models/Qwen-QwQ-32B/.cache/huggingface/download/vocab.json.lock to s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B/.cache/huggingface/download/vocab.json.lock\n",
      "s3_model_path: s3://sagemaker-us-east-1-596899493901/models/Qwen-QwQ-32B\n"
     ]
    }
   ],
   "source": [
    "!aws s3 sync {local_model_path} {s3_model_path}\n",
    "print(\"s3_model_path:\", s3_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3 Prepare vllm start scripts\n",
    "\n",
    "Then you need to a write the vllm starting scripts for endpoint, the container will automatically use the `start.sh` as the entrypont.\n",
    "\n",
    "Please carefully modify the startup script file as needed, such as the model running parameter information. All parameters can be referenced at [https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html)\n",
    "\n",
    "Here is a simple script that pulling a model from S3 and starting a vllm server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_code_path: Qwen-QwQ-32B-251111-0235\n"
     ]
    }
   ],
   "source": [
    "endpoint_model_name = sagemaker.utils.name_from_base(model_name, short=True)\n",
    "local_code_path = endpoint_model_name\n",
    "s3_code_path = f\"s3://{default_bucket}/endpoint_code/vllm_byoc/{endpoint_model_name}.tar.gz\"\n",
    "\n",
    "%mkdir -p {local_code_path}\n",
    "\n",
    "print(\"local_code_path:\", local_code_path)\n",
    "\n",
    "with open(f\"{local_code_path}/start.sh\", \"w\") as f:\n",
    "    f.write(f\"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "# download model to local\n",
    "s5cmd sync {s3_model_path}/* /opt/ml/modelfile/\n",
    "\n",
    "\n",
    "# the start script need to be adjust as you needed\n",
    "# port needs to be $SAGEMAKER_BIND_TO_PORT\n",
    "\n",
    "python3 -m vllm.entrypoints.openai.api_server \\\\\n",
    "    --port $SAGEMAKER_BIND_TO_PORT \\\\\n",
    "    --trust-remote-code \\\\\n",
    "    --tensor-parallel-size 4 \\\\\n",
    "    --gpu-memory-utilization 0.85 \\\\\n",
    "    --max-model-len 2048 \\\\\n",
    "    --enforce-eager \\\\\n",
    "    --load-format bitsandbytes \\\\\n",
    "    --dtype=half \\\\\n",
    "    --quantization bitsandbytes \\\\\n",
    "    --swap-space 32 \\\\\n",
    "    --max-num-batched-tokens 24576 \\\\\n",
    "    --max-num-seqs 12 \\\\\n",
    "    --model /opt/ml/modelfile/ \\\\\n",
    "    --served-model-name {MODEL_ID}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen-QwQ-32B-251111-0235/\n",
      "Qwen-QwQ-32B-251111-0235/start.sh\n",
      "upload: ./Qwen-QwQ-32B-251111-0235.tar.gz to s3://sagemaker-us-east-1-596899493901/endpoint_code/vllm_byoc/Qwen-QwQ-32B-251111-0235.tar.gz\n",
      "s3_code_path: s3://sagemaker-us-east-1-596899493901/endpoint_code/vllm_byoc/Qwen-QwQ-32B-251111-0235.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!rm -f {local_code_path}.tar.gz\n",
    "!tar czvf {local_code_path}.tar.gz {local_code_path}/\n",
    "!aws s3 cp {local_code_path}.tar.gz {s3_code_path}\n",
    "print(\"s3_code_path:\", s3_code_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3 Deploy endpoint on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ModelArn': 'arn:aws:sagemaker:us-east-1:596899493901:model/Qwen-QwQ-32B-251111-0235', 'ResponseMetadata': {'RequestId': '38691a0b-c763-478e-b611-c07924be00c4', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '38691a0b-c763-478e-b611-c07924be00c4', 'strict-transport-security': 'max-age=47304000; includeSubDomains', 'x-frame-options': 'DENY', 'content-security-policy': \"frame-ancestors 'none'\", 'cache-control': 'no-cache, no-store, must-revalidate', 'x-content-type-options': 'nosniff', 'content-type': 'application/x-amz-json-1.1', 'content-length': '86', 'date': 'Tue, 11 Nov 2025 02:36:00 GMT'}, 'RetryAttempts': 0}}\n",
      "endpoint_model_name: Qwen-QwQ-32B-251111-0235\n"
     ]
    }
   ],
   "source": [
    "# Step 0. create model\n",
    "\n",
    "# endpoint_model_name already defined in above step\n",
    "\n",
    "create_model_response = sagemaker_client.create_model(\n",
    "    ModelName=endpoint_model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": CONTAINER,\n",
    "        \"ModelDataUrl\": s3_code_path\n",
    "    },\n",
    "    \n",
    ")\n",
    "print(create_model_response)\n",
    "print(\"endpoint_model_name:\", endpoint_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:596899493901:endpoint-config/Qwen-QwQ-32B-251111-0236', 'ResponseMetadata': {'RequestId': '98c12cf2-b6de-4cec-943b-4451b880735c', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '98c12cf2-b6de-4cec-943b-4451b880735c', 'strict-transport-security': 'max-age=47304000; includeSubDomains', 'x-frame-options': 'DENY', 'content-security-policy': \"frame-ancestors 'none'\", 'cache-control': 'no-cache, no-store, must-revalidate', 'x-content-type-options': 'nosniff', 'content-type': 'application/x-amz-json-1.1', 'content-length': '105', 'date': 'Tue, 11 Nov 2025 02:36:02 GMT'}, 'RetryAttempts': 0}}\n",
      "endpoint_config_name: Qwen-QwQ-32B-251111-0236\n"
     ]
    }
   ],
   "source": [
    "# Step 1. create endpoint config\n",
    "\n",
    "endpoint_config_name = sagemaker.utils.name_from_base(model_name, short=True)\n",
    "\n",
    "endpoint_config_response = sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": endpoint_model_name,\n",
    "            \"InstanceType\": INSTANCE_TYPE,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 1000,\n",
    "            # \"EnableSSMAccess\": True,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(endpoint_config_response)\n",
    "print(\"endpoint_config_name:\", endpoint_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EndpointArn': 'arn:aws:sagemaker:us-east-1:596899493901:endpoint/Qwen-QwQ-32B-251111-0236', 'ResponseMetadata': {'RequestId': '6f8c89e2-8523-465e-871c-8203a7cdf613', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '6f8c89e2-8523-465e-871c-8203a7cdf613', 'strict-transport-security': 'max-age=47304000; includeSubDomains', 'x-frame-options': 'DENY', 'content-security-policy': \"frame-ancestors 'none'\", 'cache-control': 'no-cache, no-store, must-revalidate', 'x-content-type-options': 'nosniff', 'content-type': 'application/x-amz-json-1.1', 'content-length': '92', 'date': 'Tue, 11 Nov 2025 02:36:03 GMT'}, 'RetryAttempts': 0}}\n",
      "endpoint_config_name: Qwen-QwQ-32B-251111-0236\n",
      "20251111-02:36:03 status: Creating\n",
      "20251111-02:37:04 status: Creating\n",
      "20251111-02:38:04 status: Creating\n",
      "20251111-02:39:04 status: Creating\n",
      "20251111-02:40:04 status: Creating\n",
      "20251111-02:41:04 status: Creating\n",
      "20251111-02:42:04 status: Creating\n",
      "20251111-02:43:05 status: Creating\n",
      "20251111-02:44:05 status: Creating\n",
      "20251111-02:45:05 status: Creating\n",
      "Endpoint created: Qwen-QwQ-32B-251111-0236\n"
     ]
    }
   ],
   "source": [
    "# Step 2. create endpoint\n",
    "\n",
    "endpoint_name = sagemaker.utils.name_from_base(model_name, short=True)\n",
    "\n",
    "create_endpoint_response = sagemaker_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(create_endpoint_response)\n",
    "print(\"endpoint_config_name:\", endpoint_name)\n",
    "while 1:\n",
    "    status = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)[\"EndpointStatus\"]\n",
    "    if status != \"Creating\":\n",
    "        break\n",
    "    print(datetime.now().strftime('%Y%m%d-%H:%M:%S') + \" status: \" + status)\n",
    "    time.sleep(60)\n",
    "print(\"Endpoint created:\", endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test\n",
    "\n",
    "You can invoke your model with SageMaker runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\")\n",
    "messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write a quick sort in python\"\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Message api non-stream mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-0868f757326a4acfadb8dac099486e9b', 'object': 'chat.completion', 'created': 1762829925, 'model': 'Qwen/QwQ-32B', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'reasoning_content': None, 'content': 'Okay, I need to write a quicksort function in Python. Let\\'s think about how quicksort works. It\\'s a divide-and-conquer algorithm. The steps are: pick a pivot, partition the array into elements less than the pivot and greater than the pivot, then recursively sort the subarrays.\\n\\nHmm, first, I should decide on the pivot selection. Maybe choose the last element as the pivot for simplicity. Oh right, there are different ways, but for simplicity, let\\'s go with the last element.\\n\\nWait, the function will need to sort the array in place or return a new array? Since quicksort is often implemented in-place, maybe that\\'s better. Or maybe a helper function that does the work. Let me recall the standard approach.\\n\\nTypically, you have a function like quicksort(arr, low, high). The base case is when low >= high, return. Then select pivot, partition the array around the pivot, then recurse on left and right parts.\\n\\nWait, the partitioning is crucial. How do I partition? Let\\'s see, the partition function will rearrange the array so that elements less than pivot are on the left, and greater on the right. The standard Lomuto partition scheme uses a pivot and moves elements around.\\n\\nLet me outline the steps again. Let\\'s say we pick the last element as pivot. Initialize a variable i to track the partition point. Then loop through j from low to high-1, and if arr[j] < pivot, swap with i and increment i. At the end, swap pivot with arr[i]. The pivot is now in its correct position, and the arrays before and after can be sorted.\\n\\nWait, the Lomuto partition uses this approach. Let me think about the code structure.\\n\\nAlternatively, the Hoare partition scheme might be more efficient, but maybe Lomuto is easier for a first implementation.\\n\\nAlternatively, maybe it\\'s simpler to implement it with list comprehensions for the quicksort, not using in-place partitioning. Like the function returns a sorted list.\\n\\nWait the user probably expects an in-place version, but sometimes quicksort can be written with recursion on smaller and larger elements. Let me think. For example:\\n\\ndef quicksort(arr):\\n    if len(arr) <= 1:\\n        return arr\\n    pivot = arr[len(arr)//2]\\n    left = [x for x in arr if x < pivot]\\n    middle = [x for x in arr if x == pivot]\\n    right = [x for x in arr if x > pivot]\\n    return quicksort(left) + middle + quicksort(right)\\n\\nOh right, that\\'s a different approach but also valid. It\\'s not in-place, uses more memory, but is easier to read maybe. The user might prefer this version since it\\'s concise. Let me check.\\n\\nThe question was to \"write a', 'tool_calls': []}, 'logprobs': None, 'finish_reason': 'length', 'stop_reason': None}], 'usage': {'prompt_tokens': 16, 'total_tokens': 596, 'completion_tokens': 580, 'prompt_tokens_details': None}, 'prompt_logprobs': None}\n",
      "==============================\n",
      "Okay, I need to write a quicksort function in Python. Let's think about how quicksort works. It's a divide-and-conquer algorithm. The steps are: pick a pivot, partition the array into elements less than the pivot and greater than the pivot, then recursively sort the subarrays.\n",
      "\n",
      "Hmm, first, I should decide on the pivot selection. Maybe choose the last element as the pivot for simplicity. Oh right, there are different ways, but for simplicity, let's go with the last element.\n",
      "\n",
      "Wait, the function will need to sort the array in place or return a new array? Since quicksort is often implemented in-place, maybe that's better. Or maybe a helper function that does the work. Let me recall the standard approach.\n",
      "\n",
      "Typically, you have a function like quicksort(arr, low, high). The base case is when low >= high, return. Then select pivot, partition the array around the pivot, then recurse on left and right parts.\n",
      "\n",
      "Wait, the partitioning is crucial. How do I partition? Let's see, the partition function will rearrange the array so that elements less than pivot are on the left, and greater on the right. The standard Lomuto partition scheme uses a pivot and moves elements around.\n",
      "\n",
      "Let me outline the steps again. Let's say we pick the last element as pivot. Initialize a variable i to track the partition point. Then loop through j from low to high-1, and if arr[j] < pivot, swap with i and increment i. At the end, swap pivot with arr[i]. The pivot is now in its correct position, and the arrays before and after can be sorted.\n",
      "\n",
      "Wait, the Lomuto partition uses this approach. Let me think about the code structure.\n",
      "\n",
      "Alternatively, the Hoare partition scheme might be more efficient, but maybe Lomuto is easier for a first implementation.\n",
      "\n",
      "Alternatively, maybe it's simpler to implement it with list comprehensions for the quicksort, not using in-place partitioning. Like the function returns a sorted list.\n",
      "\n",
      "Wait the user probably expects an in-place version, but sometimes quicksort can be written with recursion on smaller and larger elements. Let me think. For example:\n",
      "\n",
      "def quicksort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    pivot = arr[len(arr)//2]\n",
      "    left = [x for x in arr if x < pivot]\n",
      "    middle = [x for x in arr if x == pivot]\n",
      "    right = [x for x in arr if x > pivot]\n",
      "    return quicksort(left) + middle + quicksort(right)\n",
      "\n",
      "Oh right, that's a different approach but also valid. It's not in-place, uses more memory, but is easier to read maybe. The user might prefer this version since it's concise. Let me check.\n",
      "\n",
      "The question was to \"write a\n"
     ]
    }
   ],
   "source": [
    "from botocore.config import Config\n",
    "\n",
    "sagemaker_runtime = boto3.client('runtime.sagemaker')#, config=config)\n",
    "\n",
    "payload = {\n",
    "    \"model\": MODEL_ID,\n",
    "    \"messages\": messages,\n",
    "    \"max_tokens\": 580,\n",
    "    \"stream\": False\n",
    "}\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "text = json.loads(response['Body'].read())\n",
    "print(text)\n",
    "print(\"=\"*30)\n",
    "print(text[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.2 Message api stream mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, I need to write a quicksort function in Python. Let me think about how to approach this. Quicksort is a divide-and-conquer algorithm, right? So first, I should pick a pivot element, then partition the array into elements less than the pivot and greater than the pivot. Then recursively apply quicksort to each partition.\n",
      "\n",
      "Hmm, the first step is choosing a pivot. There are different ways to choose the pivot. Common choices are the first element, last element, or middle element. Maybe for simplicity, I'll go with the last element as the pivot. That might be easier for the code.\n",
      "\n",
      "Wait, but sometimes people use the middle element to avoid worst-case scenarios when the array is already sorted. Maybe for now, stick with the last element. The user just wants a standard implementation.\n",
      "\n",
      "So the basic steps are:\n",
      "\n",
      "1. Select pivot (last element maybe).\n",
      "2. Partition the array around the pivot, so that elements less than pivot come before, and greater come after.\n",
      "3. Recursively sort the subarrays.\n",
      "\n",
      "Let me think about how to partition. Let's see, in the partition step, I need to rearrange the array such that all elements less than the pivot are on the left and greater on the right. But the elements don't have to be in any order otherwise, except around the pivot.\n",
      "\n",
      "Wait, the partitioning can be done with a loop that moves elements around. Let me recall the standard Lomuto partition scheme. That uses the last element as pivot and swaps elements accordingly.\n",
      "\n",
      "The Lomuto partition works like this:\n",
      "\n",
      "- Set pivot as the last element.\n",
      "- Initialize a pointer i (starting at low) to track the position where elements less than pivot are placed.\n",
      "- Iterate with j from low to high-1:\n",
      "   - If arr[j] <= pivot, swap arr[j] with arr[i], increment i.\n",
      "- Finally swap arr[i] with pivot so that pivot is in its correct position.\n",
      "\n",
      "Wait, need to make sure the partitioning is correct. The pivot is placed in its correct position, and elements before are smaller, and after are larger.\n",
      "\n",
      "Alternatively, the Hoare partition might be more efficient, but Lomuto is simpler for coding purposes, perhaps. Let me try Lomuto first since I can write that quickly.\n",
      "\n",
      "So, let's outline the code:\n",
      "\n",
      "The quicksort function will be recursive. The base case: if the array has <=1 element, do nothing. Else, proceed.\n",
      "\n",
      "Let me write the function structure:\n",
      "\n",
      "def quicksort(arr, low, high):\n",
      "    if low < high:\n",
      "        # find partition index\n",
      "        pi = partition(arr, low, high)\n",
      "        # recursively sort elements before and after partition\n",
      "        quicksort(arr, low, pi-1)\n",
      "        quicksort(arr, pi+1, high)\n",
      "\n",
      "Wait, but in Python, list indices can be managed this way. However, maybe it's better to use 0-based indices here. Also, the initial call would be quicksort(arr, 0, len(arr)-1).\n",
      "\n",
      "Then the partition function. Let's define partition(arr, low, high):\n",
      "\n",
      "def partition(arr, low, high):\n",
      "    pivot = arr[high]  # choose last element as pivot\n",
      "    i = low - 1  # elements <= pivot are before i+1\n",
      "    for j in range(low, high):\n",
      "        if arr[j] <= pivot:\n",
      "            i +=1\n",
      "            arr[i], arr[j] = arr[j], arr[i]\n",
      "    # swap pivot with arr[i+1]\n",
      "    arr[i+1], arr[high] = arr[high], arr[i+1]\n",
      "    return i+1\n",
      "\n",
      "Wait, let me verify this logic. The initial i starts at low-1 (so if low=0, starts at -1). Then, for each j from low to high-1 (since high is the pivot index). When element arr[j] is <= pivot, swap it to the i's place and increment i. So all elements from 0 to i after each swap are <= pivot. \n",
      "\n",
      "At the end, the pivot is at arr[high], so we need to swap it to the position after the last of the elements <= pivot. That would be the i+1 position. Wait, the i here is after being incremented, so when all the j's are done, the correct pivot position is i+1.\n",
      "\n",
      "Yes, that makes sense. Then, the pivot is placed correctly and the function returns the index of the pivot.\n",
      "\n",
      "Putting it all together, the code should work. Wait, let me test with an example. Suppose the array is [10,7,8,9,1,5], let's see:\n",
      "\n",
      "Wait, let me try a simple example. Let's say the array is [3, 6, 8, 10, 1, 2, 1], low is 0, high is 6 (the length is 7 elements). The first pivot is 1 (the last\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write a quick sort in python\"\n",
    "}]\n",
    "payload = {\n",
    "    \"model\": MODEL_ID,\n",
    "    \"messages\": messages,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": True,\n",
    "}\n",
    "\n",
    "response = sagemaker_runtime.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "buffer = \"\"\n",
    "text=[]\n",
    "for t in response['Body']:\n",
    "    buffer += t[\"PayloadPart\"][\"Bytes\"].decode()\n",
    "    last_idx = 0\n",
    "    for match in re.finditer(r'^data:\\s*(.+?)(\\n\\n)', buffer):\n",
    "        try:\n",
    "            data = json.loads(match.group(1).strip())\n",
    "            last_idx = match.span()[1]\n",
    "            print(data[\"choices\"][0][\"delta\"][\"content\"], end=\"\")\n",
    "            text.append(data[\"choices\"][0][\"delta\"][\"content\"])\n",
    "        except (json.JSONDecodeError, KeyError, IndexError) as e:\n",
    "            pass\n",
    "    buffer = buffer[last_idx:]\n",
    "print()\n",
    "print(len(tokenizer.tokenize(\"\".join(text))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Completion api non-stream mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Write a quick sort in python<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "Okay, I need to write a quicksort algorithm in Python. Let me think about how to approach this. Quicksort is a divide-and-conquer algorithm. The general steps are choosing a pivot, partitioning the array around the pivot, then recursively sorting the subarrays.\n",
      "\n",
      "First, I should decide on how to implement the partitioning. The common method is the Lomuto partition scheme or Hoare's partition. Maybe start with the simpler Lomuto because it's easier for me to code right now. Wait, but Hoare's has better performance. Hmm. Maybe Lomuto first to get it working.\n",
      "\n",
      "Wait, let me outline the steps again. The basic idea for Lomuto is:\n",
      "\n",
      "1. Choose a pivot, usually the last element.\n",
      "2. Initialize a variable i to track the partition point.\n",
      "3. Iterate through the array from start to end-1. For each j, if elements[j] <= pivot, swap with elements[i], and increment i.\n",
      "4. After iteration, swap pivot with elements[i], so pivot is in the correct position.\n",
      "5. Recursively sort left and right partitions.\n",
      "\n",
      "Alternatively, the Hoare partition uses two pointers and might be more efficient. But perhaps start simple.\n",
      "\n",
      "Maybe start with the Lomuto partition. Let's try coding the function.\n",
      "\n",
      "Wait, but the user just wants a quicksort in Python. Let me see.\n",
      "\n",
      "The function would take a list, and maybe left and right indices? Or modify in place. Let's see.\n",
      "\n",
      "The standard recursive approach might be something like:\n",
      "\n",
      "def quicksort(arr):\n",
      "    if len(arr) <=1:\n",
      "        return arr\n",
      "    else:\n",
      "        pivot = arr[0]\n",
      "        less = [x for x in arr[1:] if x <= pivot]\n",
      "        greater = [x for x in arr[1:] if x > pivot]\n",
      "        return quicksort(less) + [pivot] + quicksort(greater)\n",
      "\n",
      "Wait that's the non-in-place version, which is easy to write, but not as efficient. The user might want the in-place version?\n",
      "\n",
      "Hmm, the problem says \"write a quick sort in python\". Perhaps the implementer wants the in-place version.\n",
      "\n",
      "But maybe better to provide both. But let's go for the standard in-place with Lomuto.\n",
      "\n",
      "Alternatively, let's proceed with the in-place version using Lomuto.\n",
      "\n",
      "Wait here's an example structure:\n",
      "\n",
      "def quicksort(arr, low, high):\n",
      "    if low < high:\n",
      "        # find pivot element such that\n",
      "        # elements smaller than pivot are on the left\n",
      "        # elements greater than pivot are on the right\n",
      "        pi = partition(arr, low, high)\n",
      "        quicksort(arr, low, pi-1)\n",
      "        quicksort(arr, pi+1, high)\n",
      "\n",
      "def partition(arr, low, high):\n",
      "    pivot = arr[high] #\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "local_model_path=\"/home/ec2-user/SageMaker/models/Qwen-QwQ-32B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path, trust_remote_code=True)\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "print(prompt)\n",
    "payload = {\n",
    "    \"model\": MODEL_ID,\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens\": 580,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "print(json.loads(response['Body'].read())[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Completion api stream mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, I need to write a quicksort function in Python. Let me think about how QuickSort works. The basic idea is to pick a pivot element, partition the array so that elements less than the pivot come before those greater than the pivot, and then recursively sort the subarrays.\n",
      "\n",
      "First, I should decide on the pivot selection. The simplest way is to choose the last element as the pivot. Maybe I can start with that. Then the partitioning step is crucial. To partition, I can use the Lomuto or Hoare scheme. Let me recall how Lomuto's partitioning works. The pivot is the last element. Initialize a pointer i which tracks the position where elements less than the pivot are placed. Then iterate with another pointer j from the start to second last element. For each element, if it's less than the pivot, swap it with the element at position i and increment i. After processing all elements, swap the pivot into its correct position.\n",
      "\n",
      "Wait, here's an example. Suppose the array is [5,3,8,4,2]. Pivot is 2 (the last element). Wait, but choosing the last element might not be the best, but for a basic implementation, it's okay. Then partitioning steps: start with i as -1? Wait no, maybe the initial i is 0. Wait let me get the steps right.\n",
      "\n",
      "Actually, in Lomuto's partition scheme, the pivot is at the end. The standard algorithm goes something like:\n",
      "\n",
      "def partition(arr, low, high):\n",
      "    pivot = arr[high]\n",
      "    i = low  # Index of smaller element\n",
      "    for j in range(low, high):\n",
      "        if arr[j] <= pivot:\n",
      "            i += 1\n",
      "            arr[i], arr[j] = arr[j], arr[i]\n",
      "    arr[i+1], arr[high] = arr[high], arr[i+1]\n",
      "    return i+1\n",
      "\n",
      "Wait, so the pivot is at the end, and after the loop, everything before i+1 is <= pivot, and the elements after are >= pivot. Then swap the pivot to its correct position.\n",
      "\n",
      "Okay, then the QuickSort function would be recursive, calling itself on the lower and upper partitions.\n",
      "\n",
      "So the main quicksort function could be:\n",
      "\n",
      "def quicksort(arr, low, high):\n",
      "    if low < high:\n",
      "        pi = partition(arr, low, high)\n",
      "        quicksort(arr, low, pi-1)\n",
      "        quicksort(arr, pi+1, high)\n",
      "\n",
      "But then, how to call it? The user might need a helper function that initializes the first parameters. So maybe have a wrapper function:\n",
      "\n",
      "def quick_sort(arr):\n",
      "    def _quicksort(low, high):\n",
      "        if low < high:\n",
      "            pi = partition(arr, low, high)\n",
      "            _quicksort(low, pi-1)\n",
      "            _quicksort(pi+1, high)\n",
      "    _quicksort(0, len(arr)-1)\n",
      "    return arr\n",
      "\n",
      "Wait, but in Python, passing the array and indices. Wait, in that case, the partition function needs to be nested inside quicksort? Or perhaps all these functions can be inside the main function.\n",
      "\n",
      "Alternatively, perhaps better to have the partition inside the quicksort's helper function. Or just have them separate. Let me structure it properly.\n",
      "\n",
      "Putting all together:\n",
      "\n",
      "Wait, maybe the code structure should be:\n",
      "\n",
      "def quicksort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    else:\n",
      "        pivot = arr[len(arr) // 2]\n",
      "        left = [x for x in arr if x < pivot]\n",
      "        mid = [x for x in arr if x == pivot]\n",
      "        right = [x for x in arr if x > pivot]\n",
      "        return quicksort(left) + mid + quicksort(right)\n",
      "\n",
      "Wait, that's a different implementation using list comprehensions, which is more concise but could be more memory-intensive because it creates new lists each time. However, it's a valid implementation and perhaps easier to write.\n",
      "\n",
      "Wait, the problem says \"write a quicksort\", so maybe the first one with partition and in-place sorting is better. The recursive method where the array is sorted in place.\n",
      "\n",
      "Hmm, but the user might want either approach, but maybe the first approach for educational purposes.\n",
      "\n",
      "So the first approach requires an in-place partitioning and uses indices. Let me code that.\n",
      "\n",
      "So the full code would look like:\n",
      "\n",
      "def quick_sort(arr):\n",
      "    def partition(low, high):\n",
      "        pivot = arr[high]\n",
      "        i = low  # index for elements <= pivot\n",
      "        for j in range(low, high):\n",
      "            if arr[j] <= pivot:\n",
      "                arr[i], arr[j] = arr[j], arr[i]\n",
      "                i += 1\n",
      "        arr[i], arr[high] = arr[high], arr[i]\n",
      "        return i\n",
      "\n",
      "    def _quicksort(low, high):\n",
      "        if low < high:\n",
      "            pi = partition(low, high)\n",
      "            _quicksort(low,\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"model\": MODEL_ID,\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = sagemaker_runtime.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "buffer = \"\"\n",
    "for t in response['Body']:\n",
    "    buffer += t[\"PayloadPart\"][\"Bytes\"].decode()\n",
    "    last_idx = 0\n",
    "    for match in re.finditer(r'^data:\\s*(.+?)(\\n\\n)', buffer):\n",
    "        try:\n",
    "            data = json.loads(match.group(1).strip())\n",
    "            last_idx = match.end()\n",
    "            # print(data)\n",
    "            print(data[\"choices\"][0][\"text\"], end=\"\")\n",
    "        except (json.JSONDecodeError, KeyError, IndexError) as e:\n",
    "            pass\n",
    "    buffer = buffer[last_idx:]\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "sess.delete_model(endpoint_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
