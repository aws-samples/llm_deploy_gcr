{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SageMaker VLLM endpoint example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Define some variables\n",
    "\n",
    "The byoc will build and store a vllm endpoint docker image in you ECR private repo (for example `sagemaker_endpoint/vllm`), you need to define the following variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "INSTANCE_TYPE = \"ml.g5.2xlarge\"\n",
    "# better to work with vllm>=v0.7.2\n",
    "VLLM_VERSION = \"v0.7.2\"\n",
    "REPO_NAMESPACE = \"sagemaker_endpoint/vllm\"\n",
    "ACCOUNT = !aws sts get-caller-identity --query Account --output text\n",
    "REGION = !aws configure get region\n",
    "ACCOUNT = ACCOUNT[0]\n",
    "REGION = REGION[0]\n",
    "if REGION.startswith(\"cn\"):\n",
    "    # this is a container mirror in cn region: https://github.com/nwcdlabs/container-mirror\n",
    "    VLLM_REPO = \"048912060910.dkr.ecr.cn-northwest-1.amazonaws.com.cn/dockerhub/vllm/vllm-openai\"\n",
    "    CONTAINER = f\"{ACCOUNT}.dkr.ecr.{REGION}.amazonaws.com.cn/{REPO_NAMESPACE}:{VLLM_VERSION}\"\n",
    "else:\n",
    "    VLLM_REPO = \"vllm/vllm-openai\"\n",
    "    CONTAINER = f\"{ACCOUNT}.dkr.ecr.{REGION}.amazonaws.com/{REPO_NAMESPACE}:{VLLM_VERSION}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the container\n",
    "\n",
    "Endpoint starting codes are in `app/`. The script will build and push to ecr. \n",
    "\n",
    "**The docker only need to be built once**, and after that, when deploying other endpoints, the same docker image can be shared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmd = f\"VLLM_REPO={VLLM_REPO} VLLM_VERSION={VLLM_VERSION} REPO_NAMESPACE={REPO_NAMESPACE} ACCOUNT={ACCOUNT} REGION={REGION} bash ./build_and_push.sh \"\n",
    "print(\"Runging:\", cmd)\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploy on SageMaker\n",
    "\n",
    "define the model and deploy on SageMaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U boto3 sagemaker transformers huggingface_hub modelscope s5cmd datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 Init SageMaker session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sess.default_bucket()\n",
    "\n",
    "sagemaker_client = boto3.client(\"sagemaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Download and upload model file\n",
    "\n",
    "Firstly, you need to prepare model weights and upload to S3. You can download from HuggingFace, ModelScope or upload your own model. \n",
    "\n",
    "If you want vllm to automatically pull the model when it starts, this step can be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = MODEL_ID.replace(\"/\", \"-\").replace(\".\", \"-\")\n",
    "local_model_path = \"./models/\" + model_name\n",
    "s3_model_path = f\"s3://{default_bucket}/models/\" + model_name\n",
    "\n",
    "%mkdir -p code {local_model_path}\n",
    "\n",
    "print(\"local_model_path:\", local_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Option 1: Global region (download from HuggingFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!huggingface-cli download --resume-download {MODEL_ID} --local-dir {local_model_path} --max-workers 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Option 2: China region  (download from ModelScope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !modelscope download --local_dir {local_model_path} {MODEL_ID} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### upload to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!s5cmd sync --concurrency 32 {local_model_path}/ {s3_model_path}/\n",
    "print(\"s3_model_path:\", s3_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3 Prepare vllm start scripts\n",
    "\n",
    "Then you need to a write the vllm starting scripts for endpoint, the container will automatically use the `start.sh` as the entrypont.\n",
    "\n",
    "Please carefully modify the startup script file as needed, such as the model running parameter information. All parameters can be referenced at [https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html)\n",
    "\n",
    "Here is a simple script that pulling a model from S3 and starting a vllm server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_model_name = sagemaker.utils.name_from_base(model_name, short=True)\n",
    "local_code_path = endpoint_model_name\n",
    "s3_code_path = f\"s3://{default_bucket}/endpoint_code/vllm_byoc/{endpoint_model_name}.tar.gz\"\n",
    "\n",
    "%mkdir -p {local_code_path}\n",
    "\n",
    "print(\"local_code_path:\", local_code_path)\n",
    "\n",
    "with open(f\"{local_code_path}/start.sh\", \"w\") as f:\n",
    "    f.write(f\"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "# download model to local\n",
    "s5cmd sync --concurrency 64 \\\n",
    "    {s3_model_path}/* /temp/model_weight\n",
    "\n",
    "\n",
    "# the start script need to be adjust as you needed\n",
    "# port needs to be $SAGEMAKER_BIND_TO_PORT\n",
    "\n",
    "python3 -m vllm.entrypoints.openai.api_server \\\\\n",
    "    --port $SAGEMAKER_BIND_TO_PORT \\\\\n",
    "    --trust-remote-code \\\\\n",
    "    --tensor-parallel-size 1 --max-model-len 65536 --enforce-eager \\\\\n",
    "    --served-model-name {MODEL_ID} \\\\\n",
    "    --model /temp/model_weight\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -f {local_code_path}.tar.gz\n",
    "!tar czvf {local_code_path}.tar.gz {local_code_path}/\n",
    "!aws s3 cp {local_code_path}.tar.gz {s3_code_path}\n",
    "print(\"s3_code_path:\", s3_code_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3 Deploy endpoint on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 0. create model\n",
    "\n",
    "# endpoint_model_name already defined in above step\n",
    "\n",
    "create_model_response = sagemaker_client.create_model(\n",
    "    ModelName=endpoint_model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": CONTAINER,\n",
    "        \"ModelDataUrl\": s3_code_path\n",
    "    },\n",
    ")\n",
    "print(create_model_response)\n",
    "print(\"endpoint_model_name:\", endpoint_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. create transform input\n",
    "\n",
    "# https://aws.amazon.com/cn/blogs/machine-learning/perform-batch-transforms-with-amazon-sagemaker-jumpstart-text2text-generation-large-language-models/\n",
    "\n",
    "transform_job_name = sagemaker.utils.name_from_base(model_name, short=True)\n",
    "\n",
    "from datasets import load_dataset\n",
    "cnn_test = load_dataset('cnn_dailymail','3.0.0',split='test')\n",
    "\n",
    "#You can specify a prompt here\n",
    "prompt = \"Briefly summarize this text: \"\n",
    "#Provide the test data and the ground truth file name\n",
    "test_data_file_name = \"articles.jsonl\"\n",
    "\n",
    "test_articles = []\n",
    "\n",
    "max_tokens = 512\n",
    "\n",
    "# We will go over each data entry and create the data in the input required format as described above\n",
    "for id, test_entry in enumerate(cnn_test):\n",
    "    article = test_entry['article']\n",
    "    # Create a payload like this if you want to have different hyperparameters for each test input\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"{prompt}{article}\"\n",
    "    }]\n",
    "    payload = {\n",
    "        \"model\": MODEL_ID,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    test_articles.append(payload)\n",
    "    if id > 512:\n",
    "        break\n",
    "\n",
    "with open(test_data_file_name, \"w\") as outfile:\n",
    "    for entry in test_articles:\n",
    "        outfile.write(\"%s\\n\" % json.dumps(entry))\n",
    "\n",
    "s3_transform_input_path = f\"s3://{default_bucket}/batch_transform_job/{transform_job_name}/input\"\n",
    "s3_transform_output_path = f\"s3://{default_bucket}/batch_transform_job/{transform_job_name}/output\"\n",
    "\n",
    "!aws s3 cp {test_data_file_name} {s3_transform_input_path}/{test_data_file_name}\n",
    "# # Uploading the data        \n",
    "# s3 = boto3.client(\"s3\")\n",
    "# s3.upload_file(test_data_file_name, output_bucket, os.path.join(output_prefix + \"/batch_input/articles.jsonl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2. create transform job\n",
    "\n",
    "response = sagemaker_client.create_transform_job(\n",
    "    TransformJobName=transform_job_name,\n",
    "    ModelName=endpoint_model_name,\n",
    "    MaxConcurrentTransforms=32,\n",
    "    BatchStrategy='SingleRecord',\n",
    "    TransformInput={\n",
    "        'DataSource': {\n",
    "            'S3DataSource': {\n",
    "                'S3DataType': 'S3Prefix',\n",
    "                'S3Uri': s3_transform_input_path,\n",
    "            }\n",
    "        },\n",
    "        'ContentType': 'application/jsonlines',\n",
    "        'SplitType': 'Line'\n",
    "    },\n",
    "    TransformOutput={\n",
    "        'S3OutputPath': s3_transform_output_path,\n",
    "        'Accept': \"text/csv\",\n",
    "        'AssembleWith': 'Line',\n",
    "    },\n",
    "    TransformResources={\n",
    "        'InstanceType': INSTANCE_TYPE,\n",
    "        'InstanceCount': 1,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while 1:\n",
    "    status = sagemaker_client.describe_transform_job(TransformJobName=transform_job_name)[\"TransformJobStatus\"]\n",
    "    if status != \"InProgress\":\n",
    "        break\n",
    "    print(datetime.now().strftime('%Y%m%d-%H:%M:%S') + \" status: \" + status)\n",
    "    time.sleep(60)\n",
    "print(\"Batch transform job finished:\", status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Result\n",
    "\n",
    "The result is in `s3_transform_output_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {s3_transform_output_path}/{test_data_file_name}.out .\n",
    "\n",
    "for output_line in open(f\"{test_data_file_name}.out\").readlines():\n",
    "    output = json.loads(output_line)\n",
    "    print(output[\"choices\"][0][\"message\"][\"content\"])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
