{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SageMaker VLLM endpoint example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Define some variables\n",
    "\n",
    "The byoc will build and store a vllm endpoint docker image in you ECR private repo (for example `sagemaker_endpoint/vllm`), you need to define the following variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\"\n",
    "INSTANCE_TYPE = \"ml.g6e.12xlarge\"\n",
    "# better to work with vllm>=v0.7.2\n",
    "VLLM_VERSION = \"v0.7.2\"\n",
    "REPO_NAMESPACE = \"sagemaker_endpoint/vllm\"\n",
    "ACCOUNT = !aws sts get-caller-identity --query Account --output text\n",
    "REGION = !aws configure get region\n",
    "ACCOUNT = ACCOUNT[0]\n",
    "REGION = REGION[0]\n",
    "if REGION.startswith(\"cn\"):\n",
    "    # this is a example repo port from vllm/vllm-openai, you can create your own docker image in your global region account\n",
    "    VLLM_REPO = \"public.ecr.aws/y0a9p9k0/vllm/vllm-openai\"\n",
    "    CONTAINER = f\"{ACCOUNT}.dkr.ecr.{REGION}.amazonaws.com.cn/{REPO_NAMESPACE}:{VLLM_VERSION}\"\n",
    "else:\n",
    "    VLLM_REPO = \"vllm/vllm-openai\"\n",
    "    CONTAINER = f\"{ACCOUNT}.dkr.ecr.{REGION}.amazonaws.com/{REPO_NAMESPACE}:{VLLM_VERSION}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the container\n",
    "\n",
    "Endpoint starting codes are in `app/`. The script will build and push to ecr. \n",
    "\n",
    "**The docker only need to be built once**, and after that, when deploying other endpoints, the same docker image can be shared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmd = f\"VLLM_REPO={VLLM_REPO} VLLM_VERSION={VLLM_VERSION} REPO_NAMESPACE={REPO_NAMESPACE} ACCOUNT={ACCOUNT} REGION={REGION} bash ./build_and_push.sh \"\n",
    "print(\"Runging:\", cmd)\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploy on SageMaker\n",
    "\n",
    "define the model and deploy on SageMaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U boto3 sagemaker transformers huggingface_hub modelscope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 Init SageMaker session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sess.default_bucket()\n",
    "\n",
    "sagemaker_client = boto3.client(\"sagemaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Download and upload model file\n",
    "\n",
    "Firstly, you need to prepare model weights and upload to S3. You can download from HuggingFace, ModelScope or upload your own model. \n",
    "\n",
    "If you want vllm to automatically pull the model when it starts, this step can be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = MODEL_ID.replace(\"/\", \"-\").replace(\".\", \"-\")\n",
    "local_model_path = \"./models/\" + model_name\n",
    "s3_model_path = f\"s3://{default_bucket}/models/\" + model_name\n",
    "\n",
    "%mkdir -p code {local_model_path}\n",
    "\n",
    "print(\"local_model_path:\", local_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Option 1: Global region (download from HuggingFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!huggingface-cli download --resume-download {MODEL_ID} --local-dir {local_model_path} --max-workers 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Option 2: China region  (download from ModelScope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !modelscope download --local_dir {local_model_path} {MODEL_ID} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### upload to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!s5cmd sync --concurrency 32 {local_model_path}/ {s3_model_path}/\n",
    "print(\"s3_model_path:\", s3_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3 Prepare vllm start scripts\n",
    "\n",
    "Then you need to a write the vllm starting scripts for endpoint, the container will automatically use the `start.sh` as the entrypont.\n",
    "\n",
    "Please carefully modify the startup script file as needed, such as the model running parameter information. All parameters can be referenced at [https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html)\n",
    "\n",
    "Here is a simple script that pulling a model from S3 and starting a vllm server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_model_name = sagemaker.utils.name_from_base(model_name, short=True)\n",
    "local_code_path = endpoint_model_name\n",
    "s3_code_path = f\"s3://{default_bucket}/endpoint_code/vllm_byoc/{endpoint_model_name}.tar.gz\"\n",
    "\n",
    "%mkdir -p {local_code_path}\n",
    "\n",
    "print(\"local_code_path:\", local_code_path)\n",
    "\n",
    "with open(f\"{local_code_path}/start.sh\", \"w\") as f:\n",
    "    f.write(f\"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "# download model to local\n",
    "s5cmd sync --concurrency 64 \\\n",
    "    {s3_model_path}/* /temp/model_weight\n",
    "\n",
    "\n",
    "# the start script need to be adjust as you needed\n",
    "# port needs to be $SAGEMAKER_BIND_TO_PORT\n",
    "\n",
    "python3 -m vllm.entrypoints.openai.api_server \\\\\n",
    "    --port $SAGEMAKER_BIND_TO_PORT \\\\\n",
    "    --trust-remote-code \\\\\n",
    "    --tensor-parallel-size 4 --max-model-len 65536 --enforce-eager \\\\\n",
    "    --served-model-name {MODEL_ID} \\\\\n",
    "    --model /temp/model_weight\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -f {local_code_path}.tar.gz\n",
    "!tar czvf {local_code_path}.tar.gz {local_code_path}/\n",
    "!aws s3 cp {local_code_path}.tar.gz {s3_code_path}\n",
    "print(\"s3_code_path:\", s3_code_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3 Deploy endpoint on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 0. create model\n",
    "\n",
    "# endpoint_model_name already defined in above step\n",
    "\n",
    "create_model_response = sagemaker_client.create_model(\n",
    "    ModelName=endpoint_model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": CONTAINER,\n",
    "        \"ModelDataUrl\": s3_code_path\n",
    "    },\n",
    "    \n",
    ")\n",
    "print(create_model_response)\n",
    "print(\"endpoint_model_name:\", endpoint_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1. create endpoint config\n",
    "\n",
    "endpoint_config_name = sagemaker.utils.name_from_base(model_name, short=True)\n",
    "\n",
    "endpoint_config_response = sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": endpoint_model_name,\n",
    "            \"InstanceType\": INSTANCE_TYPE,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 1000,\n",
    "            # \"EnableSSMAccess\": True,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(endpoint_config_response)\n",
    "print(\"endpoint_config_name:\", endpoint_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2. create endpoint\n",
    "\n",
    "endpoint_name = sagemaker.utils.name_from_base(model_name, short=True)\n",
    "\n",
    "create_endpoint_response = sagemaker_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(create_endpoint_response)\n",
    "print(\"endpoint_config_name:\", endpoint_name)\n",
    "while 1:\n",
    "    status = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)[\"EndpointStatus\"]\n",
    "    if status != \"Creating\":\n",
    "        break\n",
    "    print(datetime.now().strftime('%Y%m%d-%H:%M:%S') + \" status: \" + status)\n",
    "    time.sleep(60)\n",
    "print(\"Endpoint created:\", endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test\n",
    "\n",
    "You can invoke your model with SageMaker runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Hi, who are you!\"\n",
    "}]\n",
    "\n",
    "max_tokens = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Message api non-stream mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_runtime = boto3.client('runtime.sagemaker')\n",
    "\n",
    "payload = {\n",
    "    \"model\": MODEL_ID,\n",
    "    \"messages\": messages,\n",
    "    \"max_tokens\": max_tokens,\n",
    "    \"stream\": False\n",
    "}\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "print(json.loads(response['Body'].read())[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.2 Message api stream mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"model\": MODEL_ID,\n",
    "    \"messages\": messages,\n",
    "    \"max_tokens\": max_tokens,\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = sagemaker_runtime.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "buffer = \"\"\n",
    "for t in response['Body']:\n",
    "    buffer += t[\"PayloadPart\"][\"Bytes\"].decode()\n",
    "    last_idx = 0\n",
    "    for match in re.finditer(r'^data:\\s*(.+?)(\\n\\n)', buffer):\n",
    "        try:\n",
    "            data = json.loads(match.group(1).strip())\n",
    "            last_idx = match.span()[1]\n",
    "            print(data[\"choices\"][0][\"delta\"][\"content\"], end=\"\")\n",
    "        except (json.JSONDecodeError, KeyError, IndexError) as e:\n",
    "            pass\n",
    "    buffer = buffer[last_idx:]\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Completion api non-stream mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path, trust_remote_code=True)\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "payload = {\n",
    "    \"model\": MODEL_ID,\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens\": max_tokens,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "print(json.loads(response['Body'].read())[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Completion api stream mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"model\": MODEL_ID,\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens\": max_tokens,\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = sagemaker_runtime.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "buffer = \"\"\n",
    "for t in response['Body']:\n",
    "    buffer += t[\"PayloadPart\"][\"Bytes\"].decode()\n",
    "    last_idx = 0\n",
    "    for match in re.finditer(r'^data:\\s*(.+?)(\\n\\n)', buffer):\n",
    "        try:\n",
    "            data = json.loads(match.group(1).strip())\n",
    "            last_idx = match.end()\n",
    "            # print(data)\n",
    "            print(data[\"choices\"][0][\"text\"], end=\"\")\n",
    "        except (json.JSONDecodeError, KeyError, IndexError) as e:\n",
    "            pass\n",
    "    buffer = buffer[last_idx:]\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Speed test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好，用户让我帮他写一首七言律诗介绍上海。首先，我得理解七言律诗的结构，通常是八句，每句七个字，讲究平仄和对仗。然后，我要考虑上海的主要景观和象征，比如黄浦江、东方明珠、外滩等，这些都是上海的标志性景点。\n",
      "\n",
      "接下来，我需要把这些元素融入诗中，同时保持语言的优美和韵律感。第一句可以从黄浦江开始，描述它的壮阔。第二句可以引入外滩，表现出历史与现代的结合。第三、四句可以描绘江上的景象，比如船只和灯光，增加动态感。\n",
      "\n",
      "后面几句可以转向城市的繁华，比如金融中心和新天地，展示上海的多样性。最后两句要有一个总结，表达对上海的赞美和留恋。赏析部分需要解释诗中的意象和表达的情感，让读者更好地理解诗的内涵。\n",
      "\n",
      "在写赏析时，要注意用词准确，突出诗中的亮点，比如对比手法和意象的运用。同时，要保持语言流畅，避免过于生硬。整体上，诗和赏析要相辅相成，共同展现上海的魅力。\n",
      "</think>\n",
      "\n",
      "《七律·沪上》\n",
      "黄浦江头壮阔波，\n",
      "外滩古色接天罗。\n",
      "十里长街车水马，\n",
      "万家灯火竹木桥。\n",
      "金融才显峰峦险，\n",
      "新天地开世界雕。\n",
      "欲识沪城千面貌，\n",
      "不离东珠一日遥。\n",
      "\n",
      "赏析：这首作品以黄浦江、外滩等为背景，描绘了上海的繁华与壮阔。通过“十里长街车水马，万家灯火竹木桥”的描绘，生动展现了上海的都市景象。诗中“金融才显峰峦险，新天地开世界雕”的运用，对比手法巧妙，突出了上海的现代与传统。尾联则以东珠为引，表达了对上海的深深眷恋与赞美。{'id': 'chatcmpl-0c709e734ae3450c8f69d8a4736be233', 'object': 'chat.completion.chunk', 'created': 1739512712, 'model': 'deepseek-ai/DeepSeek-R1-Distill-Llama-70B', 'choices': [], 'usage': {'prompt_tokens': 16, 'total_tokens': 534, 'completion_tokens': 518}}\n",
      "\n",
      "==================================================\n",
      "Input_tokens 16 Output_tokens 518\n",
      "First token latency 0.0737 seconds\n",
      "Output speed 18.3 tokens/seconds\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "sagemaker_runtime = boto3.client('runtime.sagemaker')\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"帮我写一首七言律诗介绍上海\"\n",
    "}]\n",
    "\n",
    "payload = {\n",
    "    \"model\": MODEL_ID,\n",
    "    \"messages\": messages,\n",
    "    \"max_tokens\": 4096,\n",
    "    \"temperature\": 0.0,\n",
    "    \"stream\": True,\n",
    "    \"stream_options\": {\"include_usage\": True},\n",
    "}\n",
    "\n",
    "response = sagemaker_runtime.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "buffer = \"\"\n",
    "time_start = time.time()\n",
    "first_token_latency = 0\n",
    "output = []\n",
    "for t in response['Body']:\n",
    "    buffer += t[\"PayloadPart\"][\"Bytes\"].decode()\n",
    "    last_idx = 0\n",
    "    for match in re.finditer(r'^data:\\s*(.+?)(\\n\\n)', buffer):\n",
    "        try:\n",
    "            chunk = json.loads(match.group(1).strip())\n",
    "            last_idx = match.span()[1]\n",
    "            # print(chunk)\n",
    "            if \"usage\" in chunk:\n",
    "                print(chunk)\n",
    "                input_tokens = chunk[\"usage\"][\"prompt_tokens\"]\n",
    "                output_tokens = chunk[\"usage\"][\"completion_tokens\"]\n",
    "            if \"choices\" in data and chunk[\"choices\"][0][\"delta\"][\"content\"]:\n",
    "                if first_token_latency == 0:\n",
    "                    first_token_latency = time.time() - time_start\n",
    "                output.append(chunk[\"choices\"][0][\"delta\"][\"content\"])\n",
    "                print(output[-1], end=\"\", flush=True)\n",
    "                \n",
    "            \n",
    "\n",
    "\n",
    "        except (json.JSONDecodeError, KeyError, IndexError) as e:\n",
    "            pass\n",
    "    buffer = buffer[last_idx:]\n",
    "\n",
    "\n",
    "total_time = time.time() - time_start\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Input_tokens\", input_tokens, \"Output_tokens\", output_tokens)\n",
    "print(f\"First token latency {first_token_latency:.3} seconds\")\n",
    "print(f\"Output speed {output_tokens/(total_time-first_token_latency):.3} tokens/seconds\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
