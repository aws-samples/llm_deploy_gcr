{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a329f0",
   "metadata": {},
   "source": [
    "# deepseek-ai/deepseek-coder-6.7b-instruct LMI deployment guide\n",
    "In this tutorial, you will use LMI container from DLC to SageMaker and run inference with it.\n",
    "\n",
    "Please make sure the following permission granted before running the notebook:\n",
    "\n",
    "- S3 bucket push access\n",
    "- SageMaker access\n",
    "\n",
    "## Step 1: Let's bump up SageMaker and import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa3208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install sagemaker --upgrade  --quiet\n",
    "%pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87345ad6-cbe9-4cf5-af27-fdcf12ed683c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip show sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ac353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment\n",
    "sagemaker_default_bucket = sess.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81deac79",
   "metadata": {},
   "source": [
    "## Step 2: Start preparing model artifacts\n",
    "In LMI contianer, we expect some artifacts to help setting up the model\n",
    "- serving.properties (required): Defines the model server settings\n",
    "- model.py (optional): A python file to define the core inference logic\n",
    "- requirements.txt (optional): Any additional pip wheel need to install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15693902-6bd9-4af5-b0e4-6a289fdfb097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name=\"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
    "model_lineage=model_name.split(\"/\")[0]\n",
    "model_specific_name = model_name.split(\"/\")[1]\n",
    "cmd=f's/option.model_id=model_name/option.model_id={model_lineage}\\/{model_specific_name}/g'\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c2f5b6-3252-443d-89b9-9da9aa7fde29",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2.1 Writing serving.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dc6765-8049-4ee5-98a4-bf3c9ca2219c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Option1(Global region)\n",
    "\n",
    "- It's better to pre-download the model and upload it to S3, then use the s3url for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b011bf5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"serving.properties\", \"w\") as wf:\n",
    "    wf.write(\"engine=Python\\n\")\n",
    "    wf.write(f\"option.model_id={model_name}\\n\")\n",
    "    wf.write(\"option.rolling_batch=vllm\\n\")\n",
    "    wf.write(\"option.max_model_len=8192\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddba45c2-346e-4893-89c5-24942369d572",
   "metadata": {},
   "source": [
    "#### Option2(China region)\n",
    "\n",
    "1. Download model from Hugging face\n",
    "2. Upload model to S3 Bucket\n",
    "3. Write serving.properties using s3url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74411479-0085-4373-b5dd-8f28d2a72250",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d87853e-c2a6-470c-b15a-d11915ac254a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "# local_model_path_name = \"Qwen2-72B-Instruct\"\n",
    "local_model_path = Path(model_specific_name)\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "\n",
    "s3_model_prefix = f\"lmi/{model_specific_name}\"\n",
    "s3url=f\"s3://{sagemaker_default_bucket}/{s3_model_prefix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d867d20-8db1-4ecf-ad44-29690478e26d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!huggingface-cli download --resume-download {model_name} --local-dir {local_model_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1058bf53-03c2-4cf6-b935-3717936a22c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload model to S3\n",
    "!aws s3 cp {local_model_path} {s3url} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceaa79c-d6d5-44e3-994e-cb2813af1c86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"serving.properties\", \"w\") as wf:\n",
    "    wf.write(\"engine=Python\\n\")\n",
    "    wf.write(f\"option.model_id={s3url}\\n\")\n",
    "    wf.write(\"option.rolling_batch=vllm\\n\")\n",
    "    wf.write(\"option.max_model_len=8192\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2abe579-4c36-4ae8-a731-c13467499672",
   "metadata": {},
   "source": [
    "### Step 2.2 Writing model.py for Message API (optional) \n",
    "- In some scenarios, we need to perform some customized processing for the input or output. We could use model.py to do customization.\n",
    "- Here we use model.py to customize input formatter for better Message API format\n",
    "\n",
    "[reference link](https://docs.djl.ai/docs/serving/serving/docs/lmi/user_guides/input_formatter_schema.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f916cd90-148a-44d7-8d8b-8680e533055c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "from djl_python.huggingface import HuggingFaceService\n",
    "from djl_python.chat_completions.chat_utils import is_chat_completions_request\n",
    "from djl_python.chat_completions.chat_properties import ChatProperties\n",
    "from djl_python.inputs import Input\n",
    "from djl_python.outputs import Output\n",
    "from djl_python.encode_decode import encode, decode\n",
    "import logging\n",
    "import json\n",
    "import types\n",
    "import re\n",
    "\n",
    "_service = HuggingFaceService()\n",
    "\n",
    "\n",
    "def parse_chat_completions_request(inputs: map, is_rolling_batch: bool,\n",
    "                                   tokenizer):\n",
    "    if not hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        raise AttributeError(\n",
    "            f\"Cannot provide chat completion for tokenizer: {tokenizer.__class__}, \"\n",
    "            f\"please ensure that your tokenizer supports chat templates.\")\n",
    "    chat_params = ChatProperties(**inputs)\n",
    "    _param = chat_params.model_dump(by_alias=True, exclude_none=True)\n",
    "    _messages = _param.pop(\"messages\")\n",
    "    _inputs = tokenizer.apply_chat_template(_messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    _param[\n",
    "        \"do_sample\"] = chat_params.temperature is not None and chat_params.temperature > 0.0\n",
    "    _param[\"details\"] = True  # Enable details for chat completions\n",
    "    _param[\n",
    "        \"output_formatter\"] = \"jsonlines_chat\" if chat_params.stream else \"json_chat\"\n",
    "\n",
    "    return _inputs, _param\n",
    "\n",
    "def parse_inputs_params(input_map, item, input_format_configs):\n",
    "    if is_chat_completions_request(input_map):\n",
    "        _inputs, _param = parse_chat_completions_request(\n",
    "            input_map, input_format_configs.is_rolling_batch,\n",
    "            input_format_configs.tokenizer)\n",
    "    else:\n",
    "        _inputs = input_map.pop(\"inputs\", input_map)\n",
    "        _param = input_map.pop(\"parameters\", {})\n",
    "\n",
    "    if input_format_configs.is_rolling_batch:\n",
    "        _param[\"stream\"] = input_map.pop(\"stream\", False)\n",
    "\n",
    "    if \"cached_prompt\" in input_map:\n",
    "        _param[\"cached_prompt\"] = input_map.pop(\"cached_prompt\")\n",
    "    if \"seed\" not in _param:\n",
    "        # set server provided seed if seed is not part of request\n",
    "        if item.contains_key(\"seed\"):\n",
    "            _param[\"seed\"] = item.get_as_string(key=\"seed\")\n",
    "    if not \"output_formatter\" in _param:\n",
    "        _param[\"output_formatter\"] = input_format_configs.output_formatter\n",
    "\n",
    "    if isinstance(_inputs, list):\n",
    "        return _inputs, _param, True\n",
    "    else:\n",
    "        return [_inputs], _param, False\n",
    "    \n",
    "\n",
    "def custom_input_formatter(self, inputs: Input, tokenizer, output_formatter) -> tuple[list[str], list[int], list[dict], dict, list]:\n",
    "\n",
    "    input_data = []\n",
    "    input_size = []\n",
    "    parameters = []\n",
    "    adapters = []\n",
    "    errors = {}\n",
    "    found_adapters = False\n",
    "    batch = inputs.get_batches()\n",
    "    # only for dynamic batch\n",
    "    is_client_side_batch = [False for _ in range(len(batch))]\n",
    "    for i, item in enumerate(batch):\n",
    "        try:\n",
    "            content_type = item.get_property(\"Content-Type\")\n",
    "            input_map = decode(item, content_type)\n",
    "            _inputs, _param, is_client_side_batch[i] = parse_inputs_params(\n",
    "                input_map, item, self.input_format_configs)\n",
    "\n",
    "        except Exception as e:  # pylint: disable=broad-except\n",
    "            logging.warning(f\"Parse input failed: {i}\")\n",
    "            input_size.append(0)\n",
    "            errors[i] = str(e)\n",
    "            continue\n",
    "\n",
    "        input_data.extend(_inputs)\n",
    "        input_size.append(len(_inputs))\n",
    "\n",
    "        for _ in range(input_size[i]):\n",
    "            parameters.append(_param)\n",
    "\n",
    "    return input_data,input_size, parameters, errors, batch\n",
    "\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    \"\"\"\n",
    "    Default handler function\n",
    "    \"\"\"\n",
    "    if not _service.initialized:\n",
    "        # stateful model\n",
    "        props = inputs.get_properties()\n",
    "        print(f\"props: {props}\")\n",
    "        # props['output_formatter'] = custom_output_formatter\n",
    "        _service.initialize(props)\n",
    "        _service.parse_input = types.MethodType(custom_input_formatter, _service)\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        # initialization request\n",
    "        return None\n",
    "\n",
    "    return _service.inference(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8186126-a5b6-492a-82df-cbfa1bbb76d9",
   "metadata": {},
   "source": [
    "### Compress model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0142973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkdir mymodel\n",
    "mv serving.properties model.py mymodel/\n",
    "tar czvf mymodel.tar.gz mymodel/\n",
    "rm -rf mymodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58cf33",
   "metadata": {},
   "source": [
    "## Step 3: Start building SageMaker endpoint\n",
    "In this step, we will build SageMaker endpoint from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d955679",
   "metadata": {},
   "source": [
    "### Getting the container image URI\n",
    "\n",
    "[Large Model Inference available DLC](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a174b36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = image_uris.retrieve(\n",
    "        framework=\"djl-lmi\",\n",
    "        region=sess.boto_session.region_name,\n",
    "        version=\"v0.28.0\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11601839",
   "metadata": {},
   "source": [
    "### Upload artifact on S3 and create SageMaker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b1e5ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_code_prefix = f\"large-model-lmi/code-{model_lineage}-{model_specific_name}\"\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "code_artifact = sess.upload_data(\"mymodel.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")\n",
    "\n",
    "model = Model(image_uri=image_uri, model_data=code_artifact, role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004f39f6",
   "metadata": {},
   "source": [
    "## Step4: Create SageMaker endpoint\n",
    "\n",
    "You need to specify the instance to use and endpoint names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0e61cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.2xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(f\"lmi-model-{model_lineage}-{model_specific_name.replace('.','-')}\")\n",
    "\n",
    "model.deploy(initial_instance_count=1,\n",
    "             instance_type=instance_type,\n",
    "             endpoint_name=endpoint_name,\n",
    "             # container_startup_health_check_timeout=3600\n",
    "            )\n",
    "\n",
    "# our requests and responses will be in json format so we specify the serializer and the deserializer\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb63ee65",
   "metadata": {},
   "source": [
    "## Step 5: Test and benchmark the inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef5e1bb-db0e-4b68-b145-42d8df98ff28",
   "metadata": {},
   "source": [
    "### Message API\n",
    "Ref: https://docs.djl.ai/docs/serving/serving/docs/lmi/user_guides/chat_input_output_schema.html#message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ea3cc6-908f-4dc2-bd3b-c870852e96c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"帮我写一段快排代码\"\n",
    "# prompt = \"你好\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "response = predictor.predict({\n",
    "    \"messages\": messages,\n",
    "    \"max_tokens\":250\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc4163c-628c-468b-acdc-718af43d4746",
   "metadata": {},
   "source": [
    "### Standard schema\n",
    "Ref: https://docs.djl.ai/docs/serving/serving/docs/lmi/user_guides/lmi_input_output_schema.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee11ce2-c488-42c5-89eb-df453878c245",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae61f7-b669-4a02-97a9-85085db6632c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcef095",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"帮我写一段快排代码\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    \n",
    ")\n",
    "parameters = {\n",
    "        \"max_new_tokens\":250,\n",
    "    }\n",
    "response = predictor.predict(\n",
    "    {\"inputs\": inputs, \"parameters\": parameters}\n",
    ")\n",
    "\n",
    "print(response[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95732e49-db07-4de2-ae1d-8ed96e95f056",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c67435-fc81-42ae-85c5-608a9eb0e366",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90745745-d3c9-4dc7-8d61-0fb9bffd6ff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "\n",
    "class TokenIterator:\n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "\n",
    "            # print(line)\n",
    "            if line and line[-1] == ord(\"\\n\"):\n",
    "                self.read_pos += len(line)\n",
    "                full_line = line[:-1].decode(\"utf-8\")\n",
    "                # print(full_line)\n",
    "                line_data = json.loads(full_line.lstrip(\"data:\").rstrip(\"/n\"))\n",
    "                return line_data[\"token\"].get(\"text\", \"\")\n",
    "            chunk = next(self.byte_iterator)\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk[\"PayloadPart\"][\"Bytes\"])\n",
    "\n",
    "            \n",
    "class MessageTokenIterator:\n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "\n",
    "            # print(line)\n",
    "            if line and line[-1] == ord(\"\\n\"):\n",
    "                self.read_pos += len(line)\n",
    "                full_line = line[:-1].decode(\"utf-8\")\n",
    "                # print(full_line)\n",
    "                line_data = json.loads(full_line.lstrip(\"data:\").rstrip(\"/n\"))\n",
    "                return line_data[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n",
    "            chunk = next(self.byte_iterator)\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk[\"PayloadPart\"][\"Bytes\"])\n",
    "\n",
    "            \n",
    "def get_realtime_response_stream(sagemaker_runtime, endpoint_name, payload):\n",
    "    response_stream = sagemaker_runtime.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType=\"application/json\",\n",
    "        CustomAttributes='accept_eula=false'\n",
    "    )\n",
    "    return response_stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eb2a3a-eba2-4ba1-89e2-9baff146134e",
   "metadata": {},
   "source": [
    "#### Standard schema streaming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16918d8-35d9-4b56-ad50-73f7ab858a1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"帮我写一段快排代码\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "        \"max_new_tokens\":256,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.8,\n",
    "    }\n",
    "\n",
    "payload = {\n",
    "    \"inputs\":  inputs,\n",
    "    \"parameters\": parameters,\n",
    "    \"stream\": True ## <-- to have response stream.\n",
    "}\n",
    "\n",
    "\n",
    "response_stream = get_realtime_response_stream(smr_client, endpoint_name, payload)\n",
    "# print_response_stream(response_stream)\n",
    "for token in TokenIterator(response_stream[\"Body\"]):\n",
    "    # pass\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a4d321-8c7c-47df-8aa7-cdb6509d2434",
   "metadata": {},
   "source": [
    "#### Message Schema streaming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba40a21-caae-4731-ac56-704091cc9de6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"帮我写一段快排代码\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "payload= {\n",
    "    \"messages\": messages,\n",
    "    \"max_tokens\":250,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.8,\n",
    "    \"stream\": \"true\"\n",
    "    }\n",
    "\n",
    "response_stream = get_realtime_response_stream(smr_client, endpoint_name, payload)\n",
    "for token in MessageTokenIterator(response_stream[\"Body\"]):\n",
    "    # pass\n",
    "    print(token, end=\"\", flash=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cd9042",
   "metadata": {},
   "source": [
    "## Clean up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d674b41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
